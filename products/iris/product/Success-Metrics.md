**"North Star"** goals for vets.gov products:

- Increase the use of VA’s self-service tools
- Enable faster access to care and more timely delivery of services
- Improve the experience our users have when interacting with the VA


**Core metrics:**

**Qualitative:**

* Analyze the content and tone/emotion of their inquiry content/responses (does this relate to the issues they’re struggling with or how much they enjoy the product?)
* CSAT score feedback widget (if we customize it to include more than just the 5 stars scale)
* Additional user research to understand if the new form improves the get help experience
    * Internally with the CC agents
    * Veterans + their communities

**Quantitative:**

* Percentage of successful submissions:
    * How many visited vs how many submitted a query
    * Goal: self-service (found the knowledge) and don’t need to submit a query
    * Related to bounce rate with Google Analytics (GA)?
    * Starting baseline = Comparing with existing IRIS: Consider average monthly over the last year, and look for a 
    * Page views compared with submissions count
    * Bounce rate numbers compared with submissions count  (GA does not count successful submissions as a bounce off the page) 
    * What are the events we want to track on GA? (Successful submission, auth, file attachments, interaction vs non-interaction events. etc. - TBD) (Custom variables for successful transactions or the dropdown categories selected, are people selecting a specific dropdown value bail higher than the others)   
* Increase in mobile usage?
    * Improved inquiry form uses DSVA design standards and more mobile-friendly
* When submitted had the veteran chosen the correct query vs was it required to be rerouted internally?
    * Goal: the topic list was intuitive enough that the veteran chose the right group to get their query routed to, and it didn’t need to be rerouted
    * Technically if it needs to be rerouted but still gets answered or answered more quickly, do we consider that negative because it needed to be rerouted or positive because it was answered anyways?
        * Reduced need for agents to do follow up to get extra info from users?
        * Ask Michelle if there are metrics on how many times an inquiry got rerouted?
* CSAT score
    * Add a page after form submission with CSAT score scale?
    * 5 star scale Feedback widget on the right side of the page  (Find Locations on VA) does track to the specific page (There is a list of PRA approved questions that can be presented as part of the feedback widget)  
* (If auth) Number of users dropping off when prompted with auth vs. number of users submitting an inquiry 
* (If FAQ is intercepting the flow before the user gets to the form) How many users get their question answered from FAQs without even seeing the inquiry form?
* (If FAQ is intercepting the flow after the user fills the form and before the inquiry is submitted) How many users get their question answered from FAQs without having to submit an inquiry? 


**Supporting metrics:**

* Number of inquiries submitted (Supporting metric, possibly not directly tied to the success of the product)
    * Compare to existing IRIS?
    * Mobile vs desktop?
    * Would we consider it positive if more inquiries were submitted after TW work because the form is more user friendly and intuitive or negative because more people need help/have questions?
* Shorter SLA (or if Michelle has this metric: regardless of SLA, what is the average turnaround time on an inquiry response?)
    * Is it 5 days right now because of the volume of inquiries or because of the time it takes for agents to reroute? If we make routing simpler in the new product, does this average turnaround time for a response decrease (positive)?
* Length of time user is filling out the form
    * Bounce rate?
    * Compare to average times for filling out other similar length/type VA.gov forms (This might not necessarily indicate an improvement over the earlier status)
* Agents can answer more inquiries a day
    * The interface became more user-friendly, less re-routing, more direct questions because topic dropdown was pared down?
* Lifecycle of an inquiry - does it get resolved with the first response vs. having multiple rounds of back & forth 
* Number of calls to contact center
* Clicks on Outgoing Links from Ask a question page 
    * Forms
    * Benefits hub
    * Phone numbers
    * Other resources 

**Additional ideas to incorporate:**

* Internal metrics vs. external/user facing
* Customer satisfaction 
* Reduction in general/other submissions
    * (Sometimes users will select ‘other’ because they get confused by options and so it requires extra time to reroute a message)
* Reduction in contact center calls
    * Possibly we might not be able to directly relate this to usability improvements in the new form but something to consider potentially
* Improved usability
    * Can’t compare with previous analytics because we don’t have this distinction, but if we observe that visitation (?) and completion rates are close for the new forms that could be good
    * What is ‘improved usability’? What does this refer to?
    * Successful completion (higher completion rates?) successful routing
    * High trust factor - did my inquiry go the right place?
    * Improved accessibility
