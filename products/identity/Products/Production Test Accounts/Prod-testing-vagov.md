# Production Testing on VA.gov

Production testing should be completed before any code is deployed to production. This starts by ensuring that the product requirements phase doesn't introduce anything that might negatively impact your current implementation, application functionality, or security. Begin by conducting localhost validation to ensure that everything your app interacts with is not negatively impacted. Implement automated checks and tests, supplemented with manual testing in staging. Peer reviews by other engineers who understand your codebase and can launch and run your changes locally are critical for confidence in your product release. Finally, maintaining and continuously improving your application's logging and monitoring systems will help minimize any potential impact on your customers. While perfection is unattainable, we should strive to minimize the impact of any issues that might occur.

## Product Enhancement Confidence

Each of these areas must be completed, because they each ensure a complete coverage of how a user or bad actor may interact with your application.

### Product Requirements

During this phase the team must review and confirm all product feature requests to ensure that they are possible within the current system constraints, but more importantly that they don’t conflict with any currently being developed or production released requirements. An example of this would be if a new request came in that said all users must be able to add a second email address to their account. On face value seems fine, unless there is already a security requirement that is being enforced within downstream databases that doesn’t allow more than one email address to be present for a given user. It's up to the application team to have a solid understanding of their own app, document all inflight and implemented requirements, and serve as the subject matter experts to inform any product feature requestors when requirements may be negatively impacted.

In short, ensure that any new requirements that are being asked for, won’t cause other requirements or systems within your application to malfunction.

### Localhost testing

Engineers must be able to run the majority of their code changes on their local machine. For VA.gov this means following the local environment setup instructions within the [developer docs](https://depo-platform-documentation.scrollhelp.site/developer-docs/run-and-build-va-gov-locally). Installing ruby and the other required packages locally, pulling down the vets-api and vets-website repos, and then launching the servers for the backend and frontend of VA.gov. Once the environment is running and before applying any local code changes, ensure the current setup works and any features you may impact work currently. Once you’ve confirmed everything works prior to making any changes then you can safely introduce the new features and perform local validation. If your changes prevent the servers from running, cause error logs, or regress the application in any way, you should quickly see this and make any necessary changes. 

The classic engineering quote is “it runs on my machine” however if it doesn’t run or work on your machine, you really have no business promoting this beyond your local environment. 

### Peer review

Nearly every engineering team requires a peer review of a change by someone other than the engineer who is asking for the change. The key thing that is up to the engineer asking for the change, and the team culture should promote this, is that the reviewer/approver has run your code changes locally on their own machine and confirmed the intended change does what you intended it to do. They should also perform a few extra related visual testing to ensure anything that might be impacted has not been impacted. What really makes a team from good to great is a team who not only tests the new feature works, but that all the things that weren’t added have not regressed. This is critical to ensuring the team minimizes introducing bugs to production and reverting production deploys.

PRs should also include verification that all automated repo requirements are met and pass. This includes solid unit tests and end to end tests. As your team matures you should regression tests included in your repo that attempt to cover things that are difficult to test or consume too much time on your local machine.

### Logging and Monitoring

As much as we can try to test our changes locally, and ask everyone to run our code locally, we cannot always guarantee that downstream system dependencies will be available during our testing and perhaps not changed between the time we tested locally and deployed our changes to production. For this reason, it's very important to have both exhaustive logging that your service is working and when it's not working. You must also build in monitoring to ensure the performance of your application isn’t degraded. Logging and monitoring takes some effort from the team to implement however it's similar to building a house. You need to have a strong foundational understanding of your application, build out monitoring templates, evaluate how your monitors are working, and improve on them as much as necessary. The most common question you will hear from VA leadership when discussing a production outage is “did you have monitoring in place to detect the issue, and how long did it take for you to find out there was an issue.” The key words are “how long before you found out”, meaning you need to have logging and monitoring in place for everything that your application is responsible for. You must find the error and issue before it's reported to you from an external party.

Teams will often forget about older monitors and things can go stale. Ensure you're not relying on the alerting component of your monitors too much. It's important to be paged when a production service is degraded below an acceptable level, but you don't want to always assume your services are functioning because your monitor says it is. Just like you check on your application as you make changes because of other downstream dependencies, you also need to occasionally check that your monitors weren’t negatively impacted by changes made to your logging and monitoring platform.

### End to End Testing

Eventually your application will become so robust that you cannot reasonably test every user flow. This is where the development of end to end testing becomes required. First document every user flow your app has, including the happy and fail paths. Then create browser/synthetic tests for every user path possible. Running these tests automatically with test users in each of your staging and below environments throughout each business day will ensure that any downstream changes and local team developed changes have not broken existing functionality. Adding alerting to test run failures and even preventing deployments from happening if all tests don’t pass are excellent things to have as your team matures. End to end testing might be the most underrated application testing method, but I argue its the most effective (provided its setup correctly). If you can properly create automated tests which simulate every possible user action on a running application with a real frontend and backend, which includes calls to all downstream live running services, it should be near impossible to release a production breaking feature. These tests must run against your staging environment, running them with mocked services and inputs just isn’t enough. Bonus points go to tests that run on all supported browsers and devices.

### Security automation

I’d be remiss as a security engineer if I didn’t include security testing within your development lifecycle. Don’t over complicate this for your team, implement all the out of the box security scanning that you get from Github, locally, and from internal security teams. Block your releases on high vulnerabilities and create tickets to investigate mediums and lowers within 30 days. Step one is always to validate that the security vulnerability can be reproduced. Next, create a fix, typically this just means pulling the latest update, then let the above testing take care of the rest. If you aren’t sure that the feature you are releasing is secure, just ask for help. Don’t be the team that exposes someone's personal data, spend the 10 minutes a day to ensure your code doesn't deploy insecure code. 

## How

Sure, this is easy to say, but let's be honest, all of this takes a lot of effort to put in place. The recommended order of implementation to develop a high degree of confidence that your changes will not negatively impact your customer is as follows.

1. **Security automation** is a few steps that can be added to your repo setup in github the moment its stood up. As stated, don’t overthink it, just turn it on and review each finding. After you get through the initial review of issues you shouldn’t see more than a few a month.  
2. Rock solid **peer review** process by engineers who actually know what your code is doing. Ensure they test the code locally on their environment EVERY time, no exceptions.  
3. Develop basic **logging** that reports on errors for ALL of your features. The error messages must give you enough information to troubleshoot the problem, so start out by logging everything that you can safely log.  
   1. Ensure your team reviews all of the errors every day and creates tickets to track progress and bugs.  
4. Create some user simulated browser interaction tests within your monitoring platform, such as datadog. **End to end** tests are underrated, but provide the best coverage.  
5. **Product requirements** for a contractor team can be more difficult to fully understand so that's why it's fifth. You must make it clear that there are other apps within your repo that you could inadvertently impact. Provided everyone is transparent about this fact, then you should request that your product leadership work with other apps to review the requirements to ensure there are no breaking concerns. Then as your team develops its own product knowledge you can then commit to the product requirement section listed above.  
6. **Monitoring** is a continuous process of building and improving what you monitor and how you respond to it. Start out with some basic  
7. Last, performance/load testing should be considered. This certainly should be considered as soon as you can, but if the above are in place, performance testing shouldn’t be as required as some think it is. There are exceptions to this such as when you move a large user base over to a new feature from an old one. However if this is the case, then load testing should just be part of the product requirements. You can take actions to slowly open the flood gates to your users.  
   1. Note: this doesn’t get a dedicated section because most app teams don’t have the resources to perform this on their own. They should raise this concern to ensure it’s covered as their application becomes larger.
