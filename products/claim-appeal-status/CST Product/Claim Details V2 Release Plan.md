# Claim Details V2 Release Plan 
## Phase I: UAT 
### Planning: 
Desired date range or test duration: 4/19/24, 1 day 
Desired number of users: 3 
How you'll recruit the right production test users: Weâ€™ll be using personal and professional networks to recruit production test users.  
How you'll conduct the testing: Live, virtual sessions, giving users assigned tasks to test use cases.  
How you'll give the test users access to the product in production w/o making it live on VA.gov: feature toggle  
### Results:  
- Number of users:   
- Number of bugs identified / fixed:  
- Types of errors logged: general usability issues

 
Any UX changes necessary based on the logs, or feedback on user challenges, or VA challenges? yes/no If yes, what:

## Phase II: Staged Rollout (also known as unmoderated production testing) 
### Planning 4/22/24 - 5/15/25  
How will you make the product available in production while limiting the number of users who can find/access it: feature toggle  
What metrics-based criteria will you look at before advancing rollout to the next stage ("success criteria")?:  
<ul>
  <li>Evidence submission error rate (DataDog)</li>   
  <li>Conversion rate from evidence request to evidence submission</li>
  <li>400 LH errors (DataDog)</li>
  <li>Bounce Rates</li>
  <li>Qualitative feedback from social listening (VA Benefits Subreddit)</li>
  <li>Call center feedback</li>
</ul>
  
Links to dashboard(s) showing "success criteria" metrics: <a href = "https://docs.google.com/spreadsheets/d/14g_nD178fwJbvXMn_24uMHJUkm85xIGEa3eeXjcylig/edit?usp=sharing">CST OKRs and KPIs - 2022</a>  

### Stage A: Canary 
#### Planning  
Length of time: 2 business days
Percentage of Users (and roughly how many users do you expect this to be): 1% (500)
#### Results:  
<ul>
  <li>Evidence submission error rate (DataDog)</li>   
  <li>Conversion rate from evidence request to evidence submission</li>
  <li>400 LH errors (DataDog)</li>
  <li>Bounce Rates</li>
  <li>Qualitative feedback from social listening (VA Benefits Subreddit)</li>
  <li>Call center feedback</li>
</ul>

### Stage B: moderate  
#### Planning  
Length of time: one week  
Percentage of Users (and roughly how many users do you expect this to be): 25% 
#### Results:  
<ul>
  <li>Evidence submission error rate (DataDog)</li>   
  <li>Conversion rate from evidence request to evidence submission</li>
  <li>400 LH errors (DataDog)</li>
  <li>Bounce Rates</li>
  <li>Qualitative feedback from social listening (VA Benefits Subreddit)</li>
  <li>Call center feedback</li>
</ul>

### Stage C: High 
#### Planning  
Length of time: one week  
Percentage of Users (and roughly how many users do you expect this to be): 50% 
#### Results  
<ul>
  <li>Evidence submission error rate (DataDog)</li>   
  <li>Conversion rate from evidence request to evidence submission</li>
  <li>400 LH errors (DataDog)</li>
  <li>Bounce Rates</li>
  <li>Qualitative feedback from social listening (VA Benefits Subreddit)</li>
  <li>Call center feedback</li>
</ul>
What UX changes (if any) are necessary based on the logs, or feedback on user challenges, or VA challenges?

### Stage D: Very High 
#### Planning  
Length of time: one week  
Percentage of Users (and roughly how many users do you expect this to be): 75% 
#### Results  
<ul>
  <li>Evidence submission error rate (DataDog)</li>   
  <li>Conversion rate from evidence request to evidence submission</li>
  <li>400 LH errors (DataDog)</li>
  <li>Bounce Rates</li>
  <li>Qualitative feedback from social listening (VA Benefits Subreddit)</li>
  <li>Call center feedback</li>
</ul>
What UX changes (if any) are necessary based on the logs, or feedback on user challenges, or VA challenges?

## Go Live! 
### Planning:
Desired date: 5/15/2024
### KPIs  
<ul>
  <li>Evidence submission error rate (DataDog)</li>
  <li> Number of evidence submissions</li>
  <li>Conversion rate from evidence request to evidence submission</li>
  <li>400 LH errors (DataDog)</li>
  <li>Bounce Rates</li>
  <li>Qualitative feedback from social listening (VA Benefits Subreddit)</li>
  <li>Medallia satisfaction score</li>
  <li>Medallia Task completion rate </li>
  <li>Call center feedback</li>
</ul>

### 1-week results:  
- Number of unique users: x  
- Post-launch KPI 1 actual: xx lorem ipsum   
- Post-launch KPI 2 actual: xx lorem ipsum   
- Post-launch KPI 3 actual: xx lorem ipsum  


Any issues with VA handling/processing?: yes/no, lorem ipsum   
Types of errors logged: lorem ipsum  
Any UX changes necessary based on the logs, or feedback on user challenges, or VA challenges? yes/no If yes, what: lorem ipsum 
### 1-month results:  
- Number of unique users: x  
- Post-launch KPI 1 actual: xx lorem ipsum   
- Post-launch KPI 2 actual: xx lorem ipsum  
- Post-launch KPI 3 actual: xx lorem ipsum   
- Any issues with VA handling/processing?: yes/no, lorem ipsum   
- Types of errors logged: lorem ipsum 

 
Any UX changes necessary based on the logs, or feedback on user challenges, or VA challenges? yes/no If yes, what: lorem ipsum 
## Post-launch Questions  
To be completed once you have gathered your initial set of data, as outlined above.   
1. How do the KPIs you gathered compare to your pre-launch definition(s) of "success"?  
2. What qualitative feedback have you gathered from users or other stakeholders, if any?  
3. Which of the assumptions you listed in your product outline were/were not validated?  
4. How might your product evolve now or in the future based on these results? 
