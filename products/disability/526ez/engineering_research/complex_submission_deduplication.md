# 'Complex' de-duping (or grouping by sameness when it's hard)

'De-duping' is the catch-all term we are using to describe our high level goal of [removing duplicate 526 submissions from consideration](https://github.com/department-of-veterans-affairs/va.gov-team/issues/80624) relative to our 'untouched submission audit'. Within this document, what is actually being discussed is a subset of that problem, wherein we want to group submissions, scoped to a single veteran, by 'sameness'. More specifically, grouping submissions by sameness in the event of multiple, inconsistant differences across those submissions, i.e. 'complex' differences. These duplicate sets (**dupe-sets** for short) will then be put through a rule engine to determine which require remediation and which can be ignored.

## Axioms
- GIVIN a set of submissions, scoped to a single user
- IF one submission is identical to another across all values it can be said to be a true duplicate of that submission
- WHEN muptiple submission are identical across *all* form values, they can be said to be a true duplicate set.
- A 'dupe set' can be as small as a single submission.
- 'dupe sets' can only ever get smaller when we introduce more differences. 

**In this document, when I talk about "sameness", this is what I'm talking about.**

## Out of scope
Samness grouping, simple or complex, does not apply rules about which member of a dupe-set should be investigated and which can be ignored.  The results of 'complex' deduping will be passed to our `TimeAndStatus` sorter to make these decisions. [TODO - document this object]

## The Diff Report

**Diff Reports** are generated by the `SubmissionDifferenceReport` object, [documented here.](https://github.com/department-of-veterans-affairs/va.gov-team-sensitive/blob/master/teams/benefits/scripts/526/submission_difference_report.rb)

This is similar to a `git diff`, where we are identifying difference between two alike datasets. The sameness grouping laid out in this document, wether simple or complex, assumes we are are digesting one of these Diff reports.  At a high level, our funnel looks like this

`a set of submissions` -> `SubmissionDifferenceReport` -> `diff report` -> `(Simple || Complex)DupeSetbuilder` -> `TimeAndStatusSorter`

More information on our Audit funnel here [TODO write / add this document] 

### TL;DR on diff reports
The Diff Report returns a hash of 'key chains' and their respective data.  A key chain is an array of hash keys that describes the location of a variation within a set of 526 submissions.  For example, when capturing the variants in the following form data:

```
submissionID 1
{
  'form526' => {
    'form526' => {
      'veteran' => {
        'mailingAddress' => <variant A>
      }
    }
  }
}

submissionID 2
{
  'form526' => {
    'form526' => {
      'veteran' => {
        'mailingAddress' => <variant B>
      }
    }
  },
}

submissionID 3
{
  'form526' => {
    'form526' => {
      'veteran' => {
        'mailingAddress' => <variant B>
      }
    }
  },
}
```

The diff report would return the following keychain and data:

```ruby
{ 
  ['form526', 'form526', 'veteran', 'mailingAddress'] => {
    'variant A' => [<submission 1 id>],
    'variant B' => [<submission 2 id>, <submission 3 id>]
  }
}
```

The keychains respective values are hashes where the variation (a String) points to an array of submissions ids that contain that variant. The diff generator finds these deeply nested variations by flattening the form json and simply comparing key chains and the strings they return.

## 'Simple' de-duping (sameness grouping)
Our 'simple sameness grouper' was a first pass at processing a duplicate report and grouping the scoped submissions into groups of sameness. [TODO - link this].  This works great if there is only one variation accross all submissions of the set. However, its common for 3 or more submissions to diverge on multiple values, and in unique ways from submission to submission. 

### 'Complex' de-duping (sameness grouping when the differences are more complex)

In the example below you can see that our previous grouping of `[<submission 2 id>, <submission 3 id>]` based on `variant B` is no longer accurate because there is now a difference nested under the keychain `['form4142']['someKey']`.  This duplicate set needs to be futher broken down based on the secondary variant.

```ruby
submissionID 1
{
  'form526' => {
    'form526' => {
      'veteran' => {
        'mailingAddress' => <variant A>
      }
    }
  }
}

submissionID 2
{
  'form526' => {
    'form526' => {
      'veteran' => {
        'mailingAddress' => <variant B>
      }
    }
  },
  'form4142' => {
    'someKey' => <variant C>
  }
}

submissionID 3
{
  'form526' => {
    'form526' => {
      'veteran' => {
        'mailingAddress' => <variant B>
      }
    }
  },
  'form4142' => {
    'someKey' => <variant D>
  }
}
```

Now our duplicate report will look like this:

```ruby
{ 
  ['form526', 'form526', 'veteran', 'mailingAddress'] => {
    'variant A' => [<submission 1 id>],
    'variant B' => [<submission 2 id>, <submission 3 id>]
  }
  ['form4142'] => {
    'variant C' => [<submission 2 id>],
    'variant D' => [<submission 3 id>]
  }
}
```

In this barely-complex example, the logic breaking these submissions into three single-member dupe sets is pretty straight forward. However, when we are looking at 5, 10, even 20 submissions for a user, often many differences across many different overlapping subsets, grouping submissions by sameness quickly becomes complex. 

**NOTE:** Something to keep in mind as we ratchet up the complexity is that **dupe sets only ever get smaller**.  If we create dupe sets based on a single difference, then those already differentiated submissions will never become *more* similar by introducing new differences across forms. Using this Axiom we can begin to develop an algorithm that iterataivly breaks down dupe-sets into smaller dupe-sets into smaller dupe-sets.  

In the contrived example below, you can see how we will break down a multi-submission, multi-variation submission set into dupe sets.

```ruby
# given submissions [1,2,3,4,5,6]
{
  <user uuid> => {
    <key chain 1> => {
      <variant> => [1, 2, 3, 4]
      <variant> => [5, 6]
    },
    <key chain 2> => {
      <variant> => [2, 3, 5]
      <variant> => [1]
      <variant> => [4, 6]
    },
    <key chain 3> => {
      <variant> => [2, 3, 5]
      <variant> => [1]
      <variant> => [4, 6]
    }
  }
}
```

Note neither that the content of a given variant nor the keychain within which it was identified is imporant here. We can think of this dupe report as nothing more than groups of dupe sets with a bit of visual kruft (dupe groups?).  For example, if we wanted to reorganize this data into a table:

|             | variant   | variant | variant | variant | variant | variant | variant | variant |
|-------------|-----------|---------|---------|---------|---------|---------|---------|---------|
| key chain 1 | [1,2,3,4] | [5,6]   |         |         |         |         |         |         |
| key chain 2 |           |         | [2,3,5] | [1]     | [4,6]   |         |         |         |
| key chain 3 |           |         |         |         |         | [1,4]   | [2,3]   | [5,6]   |

Better, but still weird because tables imply column values.  Since there is no logical connection across keychains, this still isn't super useful.  The best way to represent or data independent of unnecessary context is as an array of arrays, or a group of dupe sets

```ruby
[
  [[1,2,3,4], [5,6]],
  [[2,3,5], [1], [4,6]],
  [1,4], [2,3], [5,6]],
]
```

This is exactly what the `SimpleDuplicateSetBuilder` would return, and is what we used to generate our inital best-guess number of submissions requiring remediation.  However, a closer look reveals that some of these dupe sets contain submissions that aren't true top-to-bottom duplicates!  For instance, the very first set `[1,2,3,4]` is telling us that submissions `1` and `2` have the exact same form data.  However, in the next group of dupe sets (from keychain 2) we see `2` in the first dupe set (`[2,3,5]`) and `1` in the next as a single-element dupe set.  This tells us that `1` and `2`, when considered wholistically are not actually duplicates.

Visually sorting this out into true dupe sets takes a human a few minutes, but we need to codify it.  There are many ways to do this, but the most logically simple is probably iterative. Let's step through, iteratively comparing groups of dupe sets and trying to break them down into true dupe sets.  Another way to think of it is combining our groups of dupe sets into a single, honest dupe set.  In the following example I've added letter tags (e.g. `A:[set A], B: [set B]`) so we can talk about them in pseudo code.

Since we are comparing submissions across key chains, In the following example I'll refer to 'keychain sets'. These are the same three groups of dupe sets (I really want to say dupe groups) we have already identified.  We start by comparing the first 2. To do this we will mark the first keychain set as our 'best guess so far' aka 'the set we will compare with in our loop', aka our 'comparative set'. Afterall, a single group of dupe sets / key chain set / dupe group is what we are after, so we can just arbitraily assign this one as 'closest guess' and refine from there.  Our iteration then begins with our next keychain array, so we've marked that as our 'iterative focus', aka 'the point in the loop we are at'.

```ruby
[
comparative set -->  [A:[1,2,3,4], B:[5,6]],
iterative foucs --> [C:[2,3,5], D:[1], E:[4,6]],
                     [F:[1,4], G:[2,3], H:[5,6]]
]
```

The first deep iteration of our loop would proceed as follows like this
  - Largest subset of C and A = [2,3]
  - Largest subset of C and B = [5]
  - Largest subset of D and A = [1]
  - Largest subset of D and B = []
  - Largest subset of E and A = [4]
  - Largest subset of E and B = [6]

We've now created smaller dupe sets!  The refinement of our initial best guess is our new 'best guess so far'. Here is what it looks like; `[[2,3], [5], [1], [4], 6]]`. With only one iteration we've reduced the nubmer of dupe sets to 1.  For our next iteration, we will do the same thing, using this 'best guess so far' set as our new 'comparative set'. The loop iterates to the next keychain set, marking it as our 'iterative focus'

```ruby
[
comparative set -->  [A:[2,3], B:[5], C:[1], D:[4], E:[6]],
iterative focus --> [F:[1,4], G:[2,3], H:[5,6]]
]
```
Our second deep iteration proceeds thusly:

  - Largest subset of F and A = []
  - Largest subset of F and B = []
  - Largest subset of F and C = [1]
  - Largest subset of F and D = [4]
  - Largest subset of F and E = []
  - Largest subset of G and A = [2,3] 
  - Largest subset of G and B = [] 
  - Largest subset of G and C = []
  - Largest subset of G and D = []
  - Largest subset of G and E = []
  - Largest subset of H and A = []
  - Largest subset of H and B = [5]
  - Largest subset of H and C = []
  - Largest subset of H and D = []
  - Largest subset of H and E = [6]

Our result is `[[1], [2,3], [4], [5], [6]`.  Our dupe set of submission `[2,3]` survived, meaning it is a true duplicate! We now have a group of true dupe sets for our user, which we can pass on to our rules engine, which will return 0 or 1 submissions from each dupe set that requires investigation.  More on that here [TODO - add this document]

**Note** that if we cared to, we could optimize this to not do so many worthless iterations.  However, these duplicate reports tend to only have a few key chains, and submission sets tend to top out in the teens. For this reason, we are sacrificing performance development time and grokability.

**Note** that the hard work done here is happening in application memory. small DB queries are fired, but nothing that risks locking our database, even if this script takes days to run.

**Note** if a submission has a nil value for a keychain, then it and other nil values at that keychain form their own dupe set. See the script for more details [TODO - link script]

[TODO - link the actual code]
