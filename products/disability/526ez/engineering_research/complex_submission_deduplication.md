WIP
# 'Complex' de-duping (de-duping submission groups by multiple variants)

'Complex de-duping' herin describes the process of translating a 'duplicate Report' into actionable subsets of submission IDs.  The simple 'duplicate Report' is sufficient for de-duping submissions with only one variation per form.  However, when there are multiple variations we need a bit of further processing to break our 'Duplicate Report' down into smaller duplicate sets based on all duplicate values.

## Axioms
- GIVIN a set of submissions, scoped to a single user
- IF one submission is identical to another across all values it can be said to be a true duplicate of that submission
- WHEN muptiple submission are identical across *all* form values, they can be said to be a true duplicate set.

## Out of scope
'complex' deduping does not apply rules about which member of a duplicate set should be investigated, and which can be ignored.  The results of 'complex' deduping will be passed to our `TimeAndStatus` sorter to make these decisions.

## The 'Simple' Duplicate Report
Simple Duplicate reports are generated by the `SubmissionDuplicateReport` object, [documented here.](https://github.com/department-of-veterans-affairs/va.gov-team-sensitive/blob/master/teams/benefits/scripts/526/submission_duplicate_report.rb)

The Duplicate Report returns a hash of 'key chains' and their respective data.  A key chain is an array of hash keys that describes the location of a variation within a set of 526 submissions.  For example, when capturing the variants in the following form data:

```
submissionID 1
{
  'form526' => {
    'form526' => {
      'veteran' => {
        'mailingAddress' => <variant A>
      }
    }
  }
}

submissionID 2
{
  'form526' => {
    'form526' => {
      'veteran' => {
        'mailingAddress' => <variant B>
      }
    }
  },
  'form4142' => {
    'someKey' => <variant>
  }
}

submissionID 3
{
  'form526' => {
    'form526' => {
      'veteran' => {
        'mailingAddress' => <variant B>
      }
    }
  },
}
```
The duplicate report would return the following keychain and data:
```ruby
{ 
  ['form526', 'form526', 'veteran', 'mailingAddress'] => {
    'variant A' => [<submission 1 id>],
    'variant B' => [<submission 2 id>, <submission 3 id>]
  }
}
```
The values are hashes where the variation (String) points to an array of submissions, from the relevant set, that contain that variant.

The Deduper finds these deeply nested variations by flattening the form json and simply comparing key chains and the strings they return.

### Why this isn't enough
This works great if there is only one variation accross all submissions of the set (User scope).  However, its common for 3 or more submissions to diverge on multiple values.  In the example below you can see that our previous grouping of `[<submission 2 id>, <submission 3 id>]` based on `variant B` is no longer accurate.  This duplicate set needs to be futher broken down based on the secondary variant.

```ruby
submissionID 1
{
  'form526' => {
    'form526' => {
      'veteran' => {
        'mailingAddress' => <variant A>
      }
    }
  }
}

submissionID 2
{
  'form526' => {
    'form526' => {
      'veteran' => {
        'mailingAddress' => <variant B>
      }
    }
  },
  'form4142' => {
    'someKey' => <variant C>
  }
}

submissionID 3
{
  'form526' => {
    'form526' => {
      'veteran' => {
        'mailingAddress' => <variant B>
      }
    }
  },
  'form4142' => {
    'someKey' => <variant D>
  }
}
```
Now our duplicate report will look like this:
```ruby
{ 
  ['form526', 'form526', 'veteran', 'mailingAddress'] => {
    'variant A' => [<submission 1 id>],
    'variant B' => [<submission 2 id>, <submission 3 id>]
  }
  ['form4142'] => {
    'variant C' => [<submission 2 id>],
    'variant D' => [<submission 3 id>]
  }
}
```

It's still pretty easy to see that these three submissions must be treated as indidual 'sets'.  There are no more ignoreable duplicates.  You can probably also see how quickly this difference can become 'complex' as the number of forms and variations increase.

## 'Complex' deduping
In the contrived example below, you can see how we will break down a multi submission, multi variation duplicate report into actionable dupe sets

```ruby
# given submissions [1,2,3,4,5,6]
{
  <user uuid> => {
    <key chain 1> => {
      <variant> => [1, 2, 3, 4]
      <variant> => [5, 6]
    },
    <key chain 2> => {
      <variant> => [2, 3, 5]
      <variant> => [1]
      <variant> => [4, 6]
    },
    <key chain 3> => {
      <variant> => [2, 3, 5]
      <variant> => [1]
      <variant> => [4, 6]
    }
  }
}
```
Note neither that the content of a given variant nor the keychain within which it was identified isn't imporant here. We can think of this data as nothing more than sets of dupe sets. Given this scope, we can simplify our data to a 3 level array.

#### If we cared about the meaning of variant / key chain (we don't)

|             | variant   | variant | variant | variant | variant | variant | variant | variant |
|-------------|-----------|---------|---------|---------|---------|---------|---------|---------|
| key chain 1 | [1,2,3,4] | [5,6]   |         |         |         |         |         |         |
| key chain 2 |           |         | [2,3,5] | [1]     | [4,6]   |         |         |         |
| key chain 3 |           |         |         |         |         | [1,4]   | [2,3]   | [5,6]   |


#### Simplified for our purpose
```
[
  [[1,2,3,4], [5,6]],
  [[2,3,5], [1], [4,6]],
  [1,4], [2,3], [5,6]],
]
```

Using this simplifed version, we can step through itteratively comparing dupe sets, leaving only dupe sets present in each set of dupe sets. If we notate our bottom level arrays as such

```ruby
[
comparative set -->  [A:[1,2,3,4], B:[5,6]],
itterative foucs --> [C:[2,3,5], D:[1], E:[4,6]],
                     [F:[1,4], G:[2,3], H:[5,6]]
]
```

Then the first (deep) itteration of our loop would be something like this

1. our first dupe set-set is isolated as our 
2. Itteration begins on the remaining objects
  - Largest subset of C and A = [2,3]
  - Largest subset of C and B = [5]
  - Largest subset of D and A = [1]
  - Largest subset of D and B = []
  - Largest subset of E and A = [4]
  - Largest subset of E and B = [6]
3. The result of this itteration is our new 'most accurate' set of dupe sets: `[[2,3], [5], [1], [4], 6]]`. With only one itteration we've reduced the nubmer of dupe sets to 1.  For our next itteration, we will do the same thing, using this 'most accurate' set as our comparative set.

```ruby
[
comparative set -->  [A:[2,3], B:[5], C:[1], D:[4], E:[6]],
itterative focus --> [F:[1,4], G:[2,3], H:[5,6]]
]
```
Our second itteration proceeds thusly:

  - Largest subset of F and A = []
  - Largest subset of F and B = []
  - Largest subset of F and C = [1]
  - Largest subset of F and D = [4]
  - Largest subset of F and E = []
  - Largest subset of G and A = [2,3] 
  - Largest subset of G and B = [] 
  - Largest subset of G and C = []
  - Largest subset of G and D = []
  - Largest subset of G and E = []
  - Largest subset of H and A = []
  - Largest subset of H and B = [5]
  - Largest subset of H and C = []
  - Largest subset of H and D = []
  - Largest subset of H and E = [6]

Our result is `[[1], [2,3], [4], [5], [6]`.  Our dupe set of submission `[2,3]` survived, meaning it is a true duplicate. the other submissions will all be marked as 'invetigate' in our audit, since they are not true duplicates.


Note that if we cared to, we could optimize this to not do so many worthless itterations.  However, these Duplicate reports tend to only have a few key chains, and submission sets tend to top out in the teens. For this reason, we are sacrificing performance for time.  Also, if all goes well, this should only ever run a few times.

Note that the hard work is happening in application memory. small DB queries are fired, but nothing that risks locking our database, even if this script takes days to run.
