# Chatbot Controlled Study Research Report

#### Shane Strassberg and Rachel M. Murray, [research plan](https://github.com/department-of-veterans-affairs/va.gov-team/blob/master/products/virtual-agent/research/controlled-study/research-plan.md)


## Introduction

A well-developed and maintained virtual agent will help users solve problems and complete tasks on their own with little to no human involvement at any time of day. Ultimately, the problem we want to solve with our virtual agent is to help the Veteran self-serve and find information more easily as part of a larger Omnichannel strategy (led by VEO) to provide veterans with a seamless customer experience and access to information.  We want to start with smaller targeted content because the amount of content that is available needs to be rewritten in conversational format for the chatbot.. A proof of value (POV) was created with a preliminary set of features and content; this report details the feedback on that product.

## Research Goals

We established a number of goals around the product and how people might use it. There are many ways of understanding the success of a product, but for this study we wanted to learn how the chatbot performed and how it was ‘seen’ as being able to perform, since accuracy shapes how people trust a product. We also wanted to examine how personality can reinforce trust, and how all of these affected the larger brand relationship participants have with the VA support channels - the ultimate goal to understand if participants would use the chatbot again.

The goals of this study included:

A. Accuracy: 
  - Understand how well the chatbot performed (answered questions accurately, understood what participants were trying to ask, and if participants had information needed to take the next step to complete a task
  - Understand if and how poor performance affected participants

B. Trust:
  - Understand if people preferred to sign in (get answers to personal questions) or not to sign in (even if it means no personalized answers) and if they trusted the chatbot

C. Personality:
  - Understand how participants felt about the voice and tone of the chatbot

D. Product usage:
  - Understand how often participants speak with a VA help desk/contact center, reasons why and how a chatbot can reduce how often they call the VA
Understand if participants are willing to use the chatbot again

## Research Methodology

We launched with an unmoderated controlled study of 50 participants and were able to recruit 44 in total. Participants were invited by Perigean to participate in the research study by email and have a number of research repositories that captured the data. 

- Participants were sent a link to the chatbot on Staging.va.gov and were able to engage with it. These are the ‘chatbot log transcripts’ which are reflected in the Metrics, and show the types of questions and interactions with the product.
- Participants were also sent a link to a questionnaire on Optimal Workshop. This included both quantitative questions where people were asked to rate their opinion from a preselected set of options, and qualitative questions where people were asked to provide written feedback.

## Hypotheses 

We identified a set of hypotheses which were in response to our goals, and identified data that would validate or refute them.

A. Accuracy:
- We will see a high number of questions that the bot has not yet been trained to answer. 
- Veterans will expect that the chatbot cannot answer in-depth questions. 

B. Trust:
- More Veterans will attempt to find information that requires authentication (e.g., claims-status). 
- Veterans who may be seeking anonymity (e.g., LGBTQ+, housing or food insecurity, mental-health crisis) may be more comfortable utilizing this tool over speaking with a human.

C. Personality:
- Veterans will want a more professional tone than a more ‘friendly’ personable tone.

D. Product usage:
- Caretakers (likely more women) may give a higher satisfaction rating, or be more interested in a virtual agent/chatbot. 
- Veterans will want the chatbot to transfer their conversation directly to a human agent. 


************TO UPDATE**********

## Participants

* Age: 
   * 18-24: 1
   * 25-34: 6
   * 35 - 44: 8
   * 45 - 54: 12
   * 55-64: 6
   * over 65: 8
   * participant no age stated: 3

* Education:
   * High school diploma or equivalent (ex: GED): 2
   * Some college (no degree): 16
   * Associate’s degree / trade certificate / vocational training: 6
   * Bachelors: 12
   * Graduate degree (Masters, PhD): 7
   * Blank: 1

* Gender:  
   * Women: 19
   * Men: 24
   * Transgender: 1

* Location:
   * AK, AR, CA, CO, FL, GA, HI, ID, IN, MD, MI, MN, NC, NY, PA, PR, SC, TN, TX, UT, VA, WY

* Race:
   * American Indian or Alaska Native: 1
   * Asian: 2
   * Black or African American: 3
   * Hispanic, Latino, or Spanish origin: 2
   * White or Caucasian: 31
   * Other: 1
   * Prefer not to answer / blank: 4

We divided participants into 10 segments, with a goal of 10 participants in each:

Group | Segment and number of participants
------------ | -------------
Veterans | Segment #1: 10 participants who are female Veterans across age brackets and conflicts/periods of service - Korean Conflict, Vietnam Era, Persian Gulf, Afghanistan, Iraq.  We were able to recruit 10 out of 10 of this segment.
Veterans | Segment #2: 10 participants who are male Veterans  across age brackets and conflicts/periods of service - Korean Conflict, Vietnam Era, Persian Gulf, Afghanistan, Iraq.  We were able to recruit 9 out of 10 of this segment.
Non-Veteran | Segment #3: 10 participants who are people close to Veterans - female caretakers, male caretakers or family members (i.e. dependents).  We were able to recruit 7 out of 10 of this segment.
Usage related | Segment #4: 10 participants who are new to va.gov (2 years or less)  We were able to recruit 3 out of 10 of this segment.
Usage related | Segment #5: 10 participants who are casual, infrequent users of va.gov (once a year).  We were able to recruit 8 out of 10 of this segment.
Usage related | Segment #6: 10 participants who are frequent users (dependent on claims etc. or who visit va.gov daily or weekly). We were able to recruit 7 out of 10 of this segment.
Location | Segment #7: 10 participants who live in urban centers. We were able to recruit 1 out of 10 of this segment.
Location | Segment #8: 10 participants who live in suburban or rural areas.  We were unable to recruit for this segment.
Marginalized populations | Segment #9: 10 participants who identify as LGBTQ+.  We were able to recruit 1 out of 10 of this segment.
Marginalized populations| Segment #10: 10 participants who are experiencing economic insecurity - (i.e. experiencing homelessness/housing insecure, food insecurity, either currently or previously in their time as a Veteran post-discharge from service). We were unable to recruit for this segment.

## Key Findings

We identified 6 key findings:

A. Accuracy: 
- The chatbot was rated as being fairly accurate, although the level of accuracy left some participants feeling frustrated.

B. Trust:
- Participants understood the chatbot wasn’t a human but still felt they could trust it.
- Participation from marginalized populations was low and understanding their needs will be key to ensuring trust is built into the heart of the product.

C. Personality:
- Participants reacted positively to the personality of the chatbot in terms of voice and tone.

D. Product usage:
- Participants experience challenges with current VA customer support channels, and a VA chatbot was viewed as an opportunity to avoid those challenges.
- Overall users indicated a willingness to not only try the chatbot, but to use it again, and seemed excited about the product, how it could evolve and how it might help them and the VA.

## Details of Findings

A. Accuracy

- The chatbot was rated as being fairly accurate, although the level of accuracy left some participants feeling frustrated.
  - We hypothesized that we would see a high number of questions that the bot has not yet been trained to answer, and that Veterans will expect that the chatbot cannot answer in-depth questions. 
  - While we had the chat logs to monitor what questions were being asked, we also created a number of questions on accuracy including if the participants felt they had information to take the next step, if quality was satisfactory and if issues of poor performance affect participants.  In question 1 (“The chatbot answered my questions accurately”), participants provided feedback that 16% (7 out of 44) said the chatbot was accurate all the time, and 45% (20 out of 44) said most of the time it was accurate.  Many understood the limitations of the early version of this product in terms of what content was available and what functionality was available.  
 
> *“I understand chatboxes have limited responses, as long as it points me in the right direction then that’s fine” (Segment 1, Participant 3)*

  - In question 2 (“The chatbot understood what I was trying to ask”) participants provided feedback that 57% (25 out of 44) said the chatbot understood them with 36% (16 out of 46) saying it somewhat understood them; participants varied their language to help the chatbot understand, although this can be a frustrating experience.
  
> *“I felt a little frustrated because I thought my questions were phrased in a very clear manner with key words that the chatbot should be programmed to respond to. For example, I asked about mental health resources and the chatbot responded with information about homelessness” (Segment 3, participant 9)*

> *“It answered every one with clear concise answers. The only one that it didn't answer was about where I could find a telephone directory to my local va but it said it didn't have an answer. It didn't bother me at all because, first, I realized I didn't put the where when I said local and plus... there are way too many numbers associated with the VA's!” (Segment 1, Participant 6)*

  - In question 3 (“After reading the response, did you have the information you needed to take the next step to complete your task?”) 33 out of 46 (75%) said they had enough information to take the next step.

> *“I think it was quite excellent. The links were right on point and fit the answer perfectly! I was very impressed!” (Segment 1, Participant 6)*

  - In question 8 (‘How do you feel about the quality of the response you received? Quality refers to your satisfaction with the accuracy of responses - satisfaction with the accuracy of responses, if the right link was provided and their question answered”), 64% (28 out of 44) felt the quality was positive and 34% (16 out of 44) felt quality was negative.

> *“I am satisfied with the bot. If it didn’t directly answer my question, it provided a link to where I could look for the answer on another page“ (Segment 5, Participant 3)*

> *“I liked the answers I was given - I feel like if I have a very focused question I would probably be more comfortable calling on the phone and talking to someone in person, but to find out quick and general info I would use the chatbot” (Segment 1, participant 10)*

  - Ultimately the issue of accuracy affected how people felt. In question 4 (‘If the chatbot wasn’t able to answer your question, how did that make you feel?”) poor performance meant that participants felt negative about the experience.

> *The unpredictability of the results was surprising: “It was frustrating. The chatbot was right on sometimes, while others it was completely off topic”(Segment 5, participant 9)*

> *The success of this channel should try to be an improvement on the challenges from other channels. “More frustrated. It's hard to get answers from the VA. A chatbot should know a little more in my opinion“ (Segment 1, Participant 5)*

B. Trust

- Participants understood the chatbot wasn’t a human but still felt they could trust it. 
  - We hypothesized that more Veterans will attempt to find information that requires authentication (e.g., claims-status). In question 5 (“The chatbot can answer both general questions (for example, location of a VA facility) and personal questions (your benefit status). However, you must sign into va.gov for the chatbot to answer personal questions. How do you prefer to use this chatbot?”), 77% participants (34 out of 44) indicated they wanted to authenticate in order to get answers to personal questions. 16% (7 out of 46) would not sign in and 11% (5 out of 46) indicated they did not know. Participants validated the idea of signing in and having a personalized experience as being desirable.

> *“I thought the voice was friendly but very direct. I think I would've preferred a more personal approach in the beginning, especially if I'm signed in to VA.gov. For example, when I type my first question or open the chatbot, I would want some type of acknowledgement, "Hi, NAME. How can I help you today?" or "Thanks for your question, NAME. Let me get you some more information on that" (Segment 3, participant 9)*

  - Using an automated non-human entity can raise issues of trust especially around personal information, but participants indicated they were willing to sign in and it being non-human wasn’t preventing usage, which indicates a level of trust with the product.

> *“It was a normal response. Non-Human as I expected. I really didn't expect it to be an actual person, so I did not really have expectations'' (Segment 1, participant 5)*

- Participation from marginalized populations was low and understanding their needs will be key to ensuring trust is built into the heart of the product in the future.

  - We hypothesized that some Veterans might seek a channel where being anonymous was a key feature - for example LGBTQ+ or those experiencing economic insecurity (i.e. housing or food insecurity, mental-health crisis) - and that using a text-based tool would be more comfortable than speaking with a human.

  - Unfortunately we were unable to have a large enough representative sample from these segments - we were able to recruit only 1 participant who identifies as LGBTQ+ and 0 participants who are experiencing economic insecurity.  Individuals from other segments may have identified as LGBTQ+ or as someone experiencing economic insecurity, but identifying them as a unique segment is something we wanted to do to understand how to measure if we met our goal.  This is one area where more research is needed - similar to the research to ensure Veterans with a disability can use the chatbot, we want to ensure other marginalized voices are considered in this work. 

> *“Thanks for trying to make things easier but some veterans don't have access to internet or are uninformed on benefits available to them. I have talked to a few other veterans who were clueless about what the VA can do” (Segment 1, Participant 2)*

> *“I've had some unique problems accessing VA care. I lived in a rural area and had problems with choice and getting community care. When I moved to a new state it took the VA a year just to get me into the system, and they still can't see my other records. I have an ER bill from a year ago when I went in for covid pneumonia and the VA still hasn't paid it. I recently had a discharge upgrade and became eligible for tricare, and no one can tell me how that effects my VA usage. I recently had a specialist say that they couldn't see me and that community care had been suspended, so I couldn't get the help the ER told me to get at the VA. I've worried about how my dependents can get information on VA services if I died from covid. In the past I had a personal issue with a VA provider and a VA policeman and didn't know who to talk to about that” (Segment 5, Participant 9)*

> *“The chatbot should more readily acknowledge when it cannot answer a question, rather than trying to answer it anyway. The fact that all questions about transgender/trans people or LGBTQ people gave the same response, which was only tangentially related because it mentioned gender reassignment surgery, felt ignorant and unhelpful. Instead of listing things that are NOT covered, it should list things that ARE covered first. Responses should be expanded to talk about hormone replacement therapy (HRT) coverage, therapy/counseling/support group options, and legal assistance related to trans and LGBTQ issues” (Segment 9, participant 8)*

> *In response question 4 (“if the chatbot wasn’t able to answer your question, how did that make you feel?”), one participant noted, “Like the chatbot wasn't made with LGBTQ people in mind, or like the people who created it weren't as worried about the concerns of LGBTQ people on principle” (Segment 9, Participant 1)*


C. Personality

- Participants reacted positively to the personality of the chatbot in terms of voice and tone.

  - We hypothesized that Veterans will want a more professional tone than a more ‘friendly’ personable tone.

  - In question 6 (“How do you feel about the voice of the response you received? Voice refers to the personality of the chatbot (friendly, chatty, dry, etc.)”), 55% of participants (24 out of 44) felt positively about the voice, while 20% (9 out of 44) felt neutral, and 11% (5 out of 44) felt negative. 17% of participants (8 out of 44) didn't understand the term voice (18%)didn't understand the term voice.  While some participants felt the chatbot was a little ‘cold’, many appreciated the voice.  Others didn’t mention that the personality was less important - getting the answers was the primary goal.

> *“I don't like overly friendly chatbots. It's just weird. This one just gets you answers, and I liked that” (Segment 5, Participant 9)*

> *“I do not have an opinion about the voice; the content of the answers is all that matters” (Segment 5, Participant 4)*


- In question 7 (“How do you feel about the tone of the response you received? Tone refers to the style of response (professional, bureaucratic, casual, etc.”), 61% (27 out of 44) felt positively about the tone, 18% (8 out of 44) felt neutral, and 14% (6 out of 44) felt negative. 11% (5 of 44) didn't understand the term voice.

  - The majority of participants appreciated the professional tone - while some indicated it felt a bit bureaucratic, many felt that the chatbot struck the right balance. Striking the balance of something both professional and personal can be difficult, especially because the chatbot team tried to consciously focus on getting the right information as quickly as possible to users without extended conversation.

> *“It was very professional/matter of fact but not rude at all. They know that the answers they are giving you could possibly save a life” (Segment 1, Participant 6)*

> *“I thought the tone was professional and informative. I wouldn't change too much about the tone” (Segment 3, Participant 9)*

- Earlier in April, we conducted a smaller branding study with 16 participants (Veterans and caregivers). We tested a similar hypothesis: users would prefer a virtual agent that was “professional, polite and service driven that used plain language similar to existing VA.gov content”. 

  - The results of our earlier April interviews align with our key findings from this current study. 

  - The earlier interviews had participants who were speaking from experience when dealing with chatbots. Participants of this current study, however, had the benefit of interacting with a real chatbot prototype.

  - Confirming the key findings above, the April interviewees similarly “want(ed) communication between the VA and them to be personable, curious, and empathetic.”

  - The desire for a “professional but friendly tone that projects confidence but can also display humility when it is wrong or inaccurate” was validated.

  - Finally, both studies align to the earlier finding that “Related to tone, it should be service-driven since the Vet community is broad, spanning multiple generations, and therefore different levels of tech-usage.”

  - This reiterates the need for further research on chatbot usage by segments we weren’t fully able to recruit and talk to during this study.

D. Product usage

- Participants experience challenges with current VA customer support channels, and a VA chatbot was viewed as an opportunity to avoid those challenges.

  - We hypothesized that caretakers (likely more women) may give a higher satisfaction rating, or be more interested in a virtual agent/chatbot. 

  - We didn’t have a satisfaction rating per se but rather asked them to reply (Based on your experience today, are you willing to use the chatbot again? Yes No Maybe) because we removed Medallia. 86% (38 out of 44) responded that they would be willing to use the chatbot again, so we can intuit that satisfaction was more positive than negative.

  - We also hypothesized that Veterans will want the chatbot to transfer their conversation directly to a human agent.  

  - While the majority of participants aren’t in frequent direct communication with the VA (i.e. either monthly or yearly experiences with a help desk/contact center) the problems they contact the VA about could be addressed using a chatbot, and would reduce pain points such as long wait times for commonly asked questions.

> *“It can help if I have quick questions rather than going through the automated phone tool to get the answer I'm looking for. Plus, it can help if I have questions at hours that the VA is not available.” (Segment 3, Participant 9)*

> *“The bot can answer most questions, but very targeted or individualized questions may still require a human” (Segment 4, Participant 3)*

  - In question 9 (“How often do you speak with a VA help desk/contact center to help you resolve an issue”) the majority of users speak infrequently (32% or 14 out of 44 participants speak yearly, 40% or 18 out of 44 speak every few months and 23% (10 out of 44) speak monthly.

  - In question 10 (“What were some of the reasons you have called a VA help desk/contact center in the past”), the top three reasons participants were contacting was about appointments (scheduling, rescheduling, finding them), financial, debt, benefits or the GI Bill related issues, and a variety of other issues. 

> *An example of ‘other issues’ included this example: “I have been having alot of trouble finding out my VA’s lab hours and telephone number. It's nowhere on the Fayetteville, NC Ramsey street locations website and it takes forever to get someone on the phone. No one will give me the number” (Segment 1, Participant 6)*

- In question 11 (“How can a chatbot help you reduce how often you call the VA?”) participants mentioned how a chatbot will help to reduce the wait time to speak to an agent at a call center, and how a chatbot would provide better access to resources - in this case direct access (i.e. without having to look on va.gov) and accurate resources.  

> *“It can help if I have quick questions rather than going through the automated phone tool to get the answer I'm looking for. Plus, it can help if I have questions at hours that the VA is not available” (Segment 3, Participant 9)*

> *“I think a chatbox would be extremely useful, I hate calling the VA” (Segment 1, Participant 3)*

> *“I believe the the chat box tool has great potential to answer my questions and reduce my time holding for a VA representative to answer my questions” (Segment 5, Participant 4)*

- Overall, users indicated a willingness to not only try the chatbot, but to use it again, and seemed excited about the product, how it could evolve and how it might help them and the VA.

  -In question 12 (“Based on your experience today, are you willing to use the chatbot again?”) 86% (38 out of 44) responded that they would be willing to use the chatbot again
  
  - In question 13 (“Do you have any other feedback you'd like to share?”) participants described what they felt worked and didn’t work with the current product and how the use of it would help both users as well as the VA, and in particular help ensure those with urgent needs were helped more quickly because of this channel.

> *“I thought this was an excellent experience for those of us with disabilities that can use the chatbot to find links to our questions” (Segment 1, Participant 1)*

> *“Even though I said I prefer talking with a live person I would try using the chat box in the future if it was something I didn't feel was urgent to allow the persons to help others with urgent need”. (Segment 2, Participant 3)*

> *“Once the chatbot is loaded with more information I can see it will be a big help to the veterans and the VA” (Segment 4, Participant 2)*
  
## Metrics


**Items tracked **

- Most asked about items

  - 1. VA programs and eligibility for them (36 separate engagements or instances someone asked this into the Virtual Agent) 
  - 2. Misc. medical topics and conditions (26 engagements)
  - 3. Disability related  (17 separate engagements)
 
- Outliers of what users are asking: Wide variety of topics, including appointments, claim status, education related, VA touchpoints (locations, phone numbers), topics related to marginalized populations and others.

- Times that the bot ends up in unrecognized intents: Difficult to determine

- Times the bot ends up in error states One error message was noted in the logs

- How many questions are being asked: 379 total engagements (i.e. times a person entered something into the chatbot window, or an average of 8 engagements per session per user)

- Number of users who get to the end of the conversation vs drop off: No users dropped off

**Engagement rates **

- How many users interact with the new virtual agent? 44

- How many left without interaction? 0

- How many users ask multiple questions in one session? 48 times that a user asked multiple questions

- How many ask the same question (in variations) during one session? Difficult to determine

**Satisfaction **

- How many times was the virtual agent able to answer a question? 353 (379 total engagement minus 26 not able to answer incidents)

- How many times was it not able to answer a question? 26

- How do Veterans feel about the interaction? See elsewhere in report from Optimal Workshop or raw data

- What could be improved? See Optimal Workshop or raw data and the POV final report.

- Are they likely to use this again? 82% (38 out of 44 participants) indicated they would be likely to use the chatbot again.

- Perceived human-requirements: Unable to understand

- How many users directly ask to speak with a human? 1

- How many still want to speak with a human after an interaction? 3

- How many posed a crisis-related statement/question? There were 6 engagementts that had crisis related content:

call an ambulance for me, number to the crisis line, need a doctor now, suicide hotline number, I took a lot of medications and do not feel well, notify VA of emergency

However, we are unable to determine if those were participants ‘testing’ to see how the chatbot would respond versus actual crisis situations.

**Content requests**

- What are the most asked about topics by users? See ‘most asked about items’

- What types of tasks did users ask the bot to do? Claim related tasks (‘submit a new claim, check my claim status’) and Content related tasks (‘find VA medical facilities near me’)

## Results of Hypotheses 

 The results our work for the hypotheses included:
 
A. Accuracy hypothesis:
- We will see a high number of questions that the bot has not yet been trained to answer. 
  - Results:  Results: This was valid - many questions were unable to be answered, and in general the chatbot noted it was unable to answer

- Veterans will expect that the chatbot cannot answer in-depth questions. 
  - Results: This was valid - many of the questions were not in-depth and the focus was often on benefits coverage for conditions or queries about location that might merit a ‘yes/no’ answer or basic information such as a link.

B. Trust hypothesis:
- More Veterans will attempt to find information that requires authentication (e.g., claims-status). 
  - Results:  We had a mixture of people wanting authenticated and non-authenticated content; the key finding is that in our survey the majority of users felt comfortable with authenticating to ask personalized questions

C. Personality hypothesis:
- Veterans will want a more professional tone than a more ‘friendly’ personable tone.
  - Results: we validated that a more professional tone was appropriate for this audience.

D. Product usage hypothesis:
- Caretakers (likely more women) may give a higher satisfaction rating, or be more interested in a virtual agent/chatbot. 
  - Results: We were unable to determine a quantitative satisfaction rating tied to a specific segment in PVA but rather were dependent on Optimal Workshop’s question of ‘based on your experience today, are you willing to use the chatbot again’ - Yes/No/Maybe).  Caregiver experience should be an area of future research area

- Veterans who may be seeking anonymity (e.g., LGBTQ+, housing or food insecurity, mental-health crisis) may be more comfortable utilizing this tool over speaking with a human.
  - Results: We were unable to successfully recruit among LGBTQ+, housing or those experiencing economic insecurity; more research would be needed for these segments

- Veterans will want the chatbot to transfer their conversation directly to a human agent. 
  - Results: we did not see a large number of people who asked to speak to a human agent, but this may be tied to the instructions provided in the study where people consciously noted they were participating in a chatbot study and as such may have consciously not transferred to a live agent which would end the session.

## Next Steps

Please see the final report for the Virtual Agent project for next steps, including actions to be taken for the product roadmap and additional research areas.

## Appendix

### Conversation guide
n/a

### Interview transcripts
n/a

### Pages and applications used
Pages tested: https://staging.va.gov/virtual-agent-study/ 
