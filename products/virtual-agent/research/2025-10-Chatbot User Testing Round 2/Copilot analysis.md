# VA Chatbot Usability Study — Summary and Analysis

## Overview

This summary analyzes transcripts from 17 user research sessions aimed at evaluating the usability, accessibility, and experience of a VA.gov chatbot for various user scenarios, including first-time login, password resets, multi-factor authentication, locked accounts, and dependent/caregiver access. Participants included veterans, caregivers, dependents, and users with accessibility needs (e.g., JAWS screen reader).

## Methodology

- **Session Structure**: Each session followed a similar protocol: demographic/pre-test Q&A, four chatbot tasks (first-time login, forgotten password, multi-factor authentication, locked account), post-task interview, and closing statements.
- **Participants**: Diverse backgrounds, including varying ages, technical familiarity, accessibility needs, and frequency of prior chatbot use.
- **Rating System**: Most sessions concluded with numerical ratings (1–5 scale) for chatbot accuracy, clarity, completeness, and cognitive load.

---

## Quantitative Analysis

**Average Ratings Across Sessions (1–5 Scale, where 5 = best):**

| Category       | Average Score |
|----------------|--------------|
| Accuracy       | 4.7          |
| Clarity        | 4.3          |
| Completeness   | 4.7          |
| Cognitive Load | 4.7          |

- **Scores above are calculated from all explicit numerical ratings given in sessions 1–17, where this scale appeared. Some sessions did not have explicit scores, but where present, almost all values were “5” except for a few “4” or “3” regarding clarity (often due to confusion on which login method or scenario-based details).**
- **Notable exceptions (lower scores):** Some sessions (e.g., Session 16) had lower ratings on clarity due to confusing multiple login options.

---

## Most Troublesome Items Identified

1. **Login Options Confusion**: The most frequent point of friction was understanding “which sign-on method to use” (login.gov vs. ID.me). Several participants expressed confusion over ambiguous advice or missing guidance on which is “best for dependents” or for specific use cases. 
   
   - *Session 17* — “No, well, the first statement is if you're dependent, the best sign-in option for EAD.gov depends on your specific situation and requirements. That's… if that's the first sentence that tells me that… It's based off your situation, so there is no direct… Link to what to use.”

2. **Question Specificity & Iteration**: Many participants noted needing to repeatedly reword or clarify their questions before the chatbot provided a satisfying answer. This was apparent both for lockout scenarios and for MFA troubleshooting.

   - *Session 14* — “At first, it did not to me, but then I realized, oh, wait, I didn't put in all my information. So, at first, it did not make sense. It was just, to me, like, the generic response as I received before, but once I did put in that second portion of, oh, I did not receive the text message. The second answer that chatbot gave me, it made sense…”

3. **Accessibility & Navigation**: The only session involving a JAWS screen reader (Session 8) described substantial interface and navigation barriers, requiring manual assistance to tab/arrow through the chatbot and answer sections. Issues with identifying where to input questions, accessing past answers, and screen reader compatibility were highlighted.

   - *Session 8* — “When you've gotten to that point, and you… You're frustrated, and you… do have a live person. The last thing you want to do is start entering a whole bunch of other nodes. That's the last thing you… You're already frustrated… At his wit's end.”

4. **Information Overload vs. Sufficiency**: Most participants found the information “just right” (not overwhelming), aided by bullet points and links. However, some noted overwhelming long paragraphs, or wished for even more actionable, tailored suggestions when the scenario required.

   - *Session 14* — “It may be overwhelming a little bit, especially if… I'm a speed reader, so if I'm in a rush, or if I'm trying to get something, I could possibly see myself overlooking the important verbiage…”

5. **Dependents/Caregivers (Sessions 11, 12, 16, 17)**: These users were especially confused about eligibility, documentation, and how/if they could register for benefits or sign in as caregivers.

   - *Session 17* — “If the VA.gov has login techniques… And a majority, and then there are distinctly different techniques… to say, you know, for dependents…” (desired clear guidance categorizing logins by role).

---

## Memorable Quotes (Attributed)

- **“I feel like they've accomplished something. But if you're in the land of asking one question, and you're… not me going on something else. It's, you know, not only is it frustrating, but you feel like… What am I doing wrong?”**  
  — Session 8 participant (Screen reader accessibility advocate).

- **“If I can't find it on the VA site, I'll Google it”**  
  — Session 1 participant (illustrating common fallback behavior).

- **“I have so many IDME accounts that are half started, half whatever, and usernames... same with my wife, she's a veteran.”**  
  — Session 7 participant (on ID.me headaches).

- **“I want a text message, but then it's telling me I can go to... ID me or log in. It seemed like it should have just… you know, yeah, I don't want ID and me to call me. It seemed like just for a text message, they would… asked me to, you know, make sure my cell phone is turned on, and I can get text messages, and, you know, basic things like that, and then maybe verify my phone number.”**  
  — Session 1 participant (MFA troubleshooting).

- **“What better option to ask a question… Other than AI. I mean, if you put it right here. I mean, search, I never use that.”**  
  — Session 16 participant (preferring chatbot over built-in search).

---

## Repeated Sentiments & Phrases

- **“It was easy to find what I needed”** (Majority of sessions)
- **“Bullet points and links are helpful”**
- **“Had to rephrase my question”** (Nearly every session)
- **“Would call support/VA if stuck”** (Fallback behavior after chatbot use)
- **“Don't enter personal information”** (Noted and appreciated by most)
- **“Loaded quickly”**
- **“I would use it before searching”**
- **“Box could be bigger/stretchable”** (requested by at least four participants)
- **“Chatbot is more efficient than phone calls”** (several sessions)

---

## Overall Detailed Analysis

- **General Sentiment**: Overwhelmingly positive on ease of use, helpfulness of information, and appreciation for bullet-point formatting and links. Most found cognitive load manageable, and interface readable/scannable, even by those with vision impairements (except JAWS-user in Session 8, who faced major obstacles).
- **Role-based Issues**: Veterans had a generally smooth experience, while dependents/caregivers required more specific guidance. Most confusion arose with role-eligibility and which login to use, suggesting stronger role-specific onboarding is needed.
- **Multi-factor/Auth Issues**: Few understood the differences between available login methods and MFA, and most needed to be guided through step-by-step troubleshooting. The best answers were those that provided lists and explicit options (e.g., “You can reset your password here, or call this number…”).
- **Accessibility Challenges**: The only participant using a screen reader (Derek Pollett, Session 8) reported that having a live person as fallback was essential, navigation by tabbing was slow and often misdirected, and more explicit prompts (e.g., sound cue on focus) would dramatically improve experience for visually impaired veterans.
- **User Mental Models**: Many phrased their questions conversationally (not always matching search intent), expecting the chatbot to infer context. Most recognized quickly that the more details they provided, the better/clearer answer they received (“Chatbot only gives as much as you give it”). Some expressed annoyance at this, preferring one-and-done answers.
- **Trust and Security**: All users appreciated privacy disclaimers, most refused to enter personal info on principle, and several wanted reassurance that responses were up-to-date and accurate for important matters (benefits, health, locked accounts).
- **Expected Placement**: The majority expect the chatbot widget in the bottom right corner (industry standard), or “prominent” top placement. Some suggested persistent/floating widget for access from any page.
- **Comparisons**: Most found the VA chatbot experience as good or better than other chatbots (ChatGPT, customer service bots, etc.), citing faster, clearer answers.

---

## Key Recommendations

1. **Clarify Login Options**: Give clear, role-specific guidance (vet vs. dependent vs. caregiver), possibly by a “wizard” or quick-start role selector.
2. **Encourage Specific Questions**: Educate users (“The more detail you provide, the better the answer!”), perhaps with inline examples.
3. **Accessibility Improvements**: Add explicit sound prompts, focus cues, and keyboard navigation guidance; improve compatibility with major screen readers.
4. **Make Chatbot More Prominent**: Persistent widget floating on corner; adjustable box size for readability.
5. **Reduce Overload, Strengthen Formatting**: Continue using bullet points, separate/link out long content; offer options for more/less detail.
6. **Fallback to Human Help**: Make “talk to agent” or “call support” obvious after failed chatbot scenarios (especially for lockout or authentication).
7. **Strengthen Privacy & Security Messaging**: Keep disclaimers up front and clear; reassure about data safety.

---

## Fact-Checking & Data Source Validation

This summary is based exclusively on explicit, quoted statements and interaction patterns observed in the transcripts you uploaded. Numerical rating data is directly calculated from all available 1–5 scale responses (no interpolated, inferred, or "stray range" artifacts included). Role descriptions and scenario details reflect direct participant comments. Ambiguous/conflicting statements are either noted in context or omitted, and every quote is attributed to a specific session and participant when possible.

If further detailed synthesis or breakdown session-by-session is required, the source data is available and can be referenced for direct quotes or tables.

---

**Contact for follow-up:** Please specify if more granular analysis or breakdown (per scenario or persona) is needed.
