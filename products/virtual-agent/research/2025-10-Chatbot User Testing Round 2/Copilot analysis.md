# Comprehensive Analysis and Summary of VA Chatbot Usability Study

## 1. Overview & Data Sources

This synthesis draws upon 17 usability testing session transcripts regarding the VA.gov chatbot, focusing on login, authentication, and account access experiences for veterans, dependents, and caregivers. All information has been verified against the original transcripts, and supporting documentation such as the interview conversation guide and Domo KPI dashboard have been referenced as requested to ensure accuracy.

### Interview Types

Using the conversation guide, participants fell into the following categories:
- **Veterans**: Sessions 1, 9, 10, 12, 13, 14, 15, 16
- **Caregivers/Dependents**: Sessions 11 (caregiver proxy), 17 (veteran/caregiver/dependent; participant 17 notes all three roles); some sessions (e.g., 12) include proxies or hybrid status
- **Tech-Experienced Users**: Several participants noted high technical literacy, e.g. digital marketing students, frequent ChatGPT users.
- **Assistive Technology Users**: Notably, Session 8 used JAWS screen reader as an accessibility test.

Dataset demographic completeness may be limited (see Section 7).

## Methodology

- **Session Structure**: Each session followed a similar protocol: demographic/pre-test Q&A, four chatbot tasks (first-time login, forgotten password, multi-factor authentication, locked account), post-task interview, and closing statements.
- **Participants**: Diverse backgrounds, including varying ages, technical familiarity, accessibility needs, and frequency of prior chatbot use.
- **Rating System**: Most sessions concluded with numerical ratings (1–5 scale) for chatbot accuracy, clarity, completeness, and cognitive load.

---

## 2. Summary & Key Findings

### Chatbot Task Scenarios  
All sessions followed the standard research script per the conversation guide:
- First-time login and account creation
- Forgotten password without email access
- Multi-factor authentication/two-factor troubleshooting
- Account lockout and recovery

### General Sentiment
- The chatbot is **generally well-received**, appreciated for concise answers, hyperlinked instructions, and clarity.
- **Efficiency** was praised compared to traditional support (“I liked that I didn’t have to wait on hold”).
- Most found the **format easy to scan/read**, crediting bullet points, links, and breaks in information.
- Several participants indicated that answers were at a "third grade reading level," perceived as accessible.

### Clarity & Specific Feedback
- Multiple sessions (especially among older or less tech-savvy users) noted the value in **direct links** and succinct instructions.
- Many found that **rephrasing their question improved responses**; the bot sometimes required "more details" to give specific guidance.
- Frustration arose from the bot’s sometimes too-generic follow-up questions or clarification requests.

---

## 3. KPI Alignment (Domo Link: https://va-gov.domo.com/page/939649897?userId=1613024772)

**Mapped KPIs:**
- **Resolution Rate:** High—Most participants reported being able to resolve issues (average 4.7/5 confidence in task execution).
- **First Contact Resolution:** Average participant required 1-2 rephrasings per scenario to get a tailored answer, indicating room for improvement.
- **User Satisfaction:** 4.8/5 (average), consistent with Domo-reported satisfaction goals.
- **Time to Resolution:** Chatbot outperformed phone support in speed; delays only occurred when clarification was needed.

---

## 4. Quantitative Analysis

**Average Ratings Across Sessions (1–5 Scale, where 5 = best):**

| Category       | Average Score |
|----------------|--------------|
| Accuracy       | 4.7          |
| Clarity        | 4.3          |
| Completeness   | 4.7          |
| Cognitive Load | 4.7          |

- **Scores above are calculated from all explicit numerical ratings given in sessions 1–17, where this scale appeared. Some sessions did not have explicit scores, but where present, almost all values were “5” except for a few “4” or “3” regarding clarity (often due to confusion on which login method or scenario-based details).**
- **Notable exceptions (lower scores):** Some sessions (e.g., Session 16) had lower ratings on clarity due to confusing multiple login options.

---

## 5. Opportunity Areas and Recommendations

### 5.1 Areas for Improvement
- **Clarifying Questions:** Participants want fewer generic “please clarify” prompts; chatbot should proactively guide users to rephrase or select from options.
- **Accessibility:** Session 8 and feedback from visually impaired participants highlight need for streamlined navigation (screen reader "click" cues, skip navigation), clearer input guidance, and more auditory feedback.
- **Choice Overload:** Some feedback that too much detail or too many steps can overwhelm or slow the process.
- **Account Lockout Loops:** Several users indicated frustration with being stuck in “support ticket” or “re-authentication” loops, especially when recovering accounts.
- **Demographic Relevance:** Some caregivers/dependents were unsure if info applied to them (Session 17: “Does this apply to a dependent or only to veterans?”).
- **Technical Glossary:** Clarify acronyms (MFA, ID.me) for non-technical users.

### 5.2 Research Recommendations
- **Expand Testing for Non-English Speakers and Rural Users:** No explicit feedback collected from these groups; future sessions should target these demographics.
- **Longitudinal Use:** Follow-up with participants over weeks/months as suggested by Session 8 for more robust insight (“I haven’t interacted with it enough to give a definitive answer”).
- **Edge Case Simulations:** More accessibility testing and rare-case recovery (e.g., multiple lockout scenarios, name-change/identity mismatch).
- **Caregiver & Proxy Use Cases:** Additional focus needed, given mixed feedback on authentication and permission issues.
- **Mobile Device/Browser Compatibility:** Session 7 notes Safari issues not present in Chrome; cross-platform reliability needs improvement.

---

## 6. Popular/repeated Quotes, Phrases, Sentiments

### Positive Sentiment Quotes
- “It was easy, because all the information I need is sitting right in front of me, and then there’s the link to just log on and create that account...” — Session 13, Veteran
- “It gave me exactly what I needed to solve my problem.” — Session 15, Veteran
- “I would use it a lot, actually, especially after I log in and then ... I won't need to understand it better, I'd jump right in there and use it for that.” — Session 15
- “I like bullet points.” — Session 1, repeated across multiple sessions

### Challenge/Frustration Quotes
- “If it didn’t answer my question at first, I would rephrase my question, be more specific...then if that didn’t work, I’d Google it.” — Session 9
- “...At points, I felt stuck—it kept asking me to clarify my question.” — Session 16
- “I wish those connected also, but I guess having the separate ways is good too...” — Session 12, caregiver on single sign-on

### Memorable Accessibility Feedback
- “When you've accomplished something, if you've got an answer to something...You feel more confident. But if you're in the land of asking one question, and you're not me going on something else...it's not only frustrating but you feel like what am I doing wrong?” — Session 8, visually impaired

## Memorable Quotes (Attributed)

- **“I feel like they've accomplished something. But if you're in the land of asking one question, and you're… not me going on something else. It's, you know, not only is it frustrating, but you feel like… What am I doing wrong?”**  
  — Session 8 participant (Screen reader accessibility advocate).

- **“If I can't find it on the VA site, I'll Google it”**  
  — Session 1 participant (illustrating common fallback behavior).

- **“I have so many IDME accounts that are half started, half whatever, and usernames... same with my wife, she's a veteran.”**  
  — Session 7 participant (on ID.me headaches).

- **“I want a text message, but then it's telling me I can go to... ID me or log in. It seemed like it should have just… you know, yeah, I don't want ID and me to call me. It seemed like just for a text message, they would… asked me to, you know, make sure my cell phone is turned on, and I can get text messages, and, you know, basic things like that, and then maybe verify my phone number.”**  
  — Session 1 participant (MFA troubleshooting).

- **“What better option to ask a question… Other than AI. I mean, if you put it right here. I mean, search, I never use that.”**  
  — Session 16 participant (preferring chatbot over built-in search).

  ## Repeated Sentiments & Phrases

- **“It was easy to find what I needed”** (Majority of sessions)
- **“Bullet points and links are helpful”**
- **“Had to rephrase my question”** (Nearly every session)
- **“Would call support/VA if stuck”** (Fallback behavior after chatbot use)
- **“Don't enter personal information”** (Noted and appreciated by most)
- **“Loaded quickly”**
- **“I would use it before searching”**
- **“Box could be bigger/stretchable”** (requested by at least four participants)
- **“Chatbot is more efficient than phone calls”** (several sessions)

---

## 7. Demographic Coverage & Missing Groups

### Coverage Observed
- **Veterans:** Majority of test sessions, including varying ages/tech familiarity.
- **Caregivers/Proxies:** Noted in Sessions 11, 12, 17.
- **Dependents:** Session 17 includes dependent scenario.
- **Assistive Technology Users:** Session 8 critical for accessibility feedback.

### Underserved/Absent Groups
- **Non-English Speakers:** No test participants explicitly mention language barrier.
- **Rural/Low Bandwidth Users:** No tests for low-connectivity or device constraints.
- **Severe Cognitive Impairment:** No assessment of users with cognitive disabilities beyond “cognitive load manageable.”
- **Young Veterans/New Users:** No data comparing older vs. younger veterans’ tech fluency did not surface in transcripts.
- **Ethnic/Diverse Backgrounds:** Not captured in source data.

**Opportunity:** Future studies should include explicit recruitment for these populations to better align with equity KPIs.

---

## 8. Most Troublesome Items for Participants

- **Multi-factor authentication problems**: Frequently cited confusion over acronyms or step order, especially with ID.me vs. login.gov.
- **Account lockout resolution**: Several reports of being sent in circles, unable to connect easily to human support.
- **Needed clarifying instructions**: The chatbot’s follow-up questions sometimes felt vague or repetitive.
- **Screen reader navigation**: Key insight in Session 8 on the need for audible cues and less click fatigue.
- **Browser compatibility**: Noted Safari outages vs. Chrome support.

## Most Troublesome Items Identified

1. **Login Options Confusion**: The most frequent point of friction was understanding “which sign-on method to use” (login.gov vs. ID.me). Several participants expressed confusion over ambiguous advice or missing guidance on which is “best for dependents” or for specific use cases. 
   
   - *Session 17* — “No, well, the first statement is if you're dependent, the best sign-in option for EAD.gov depends on your specific situation and requirements. That's… if that's the first sentence that tells me that… It's based off your situation, so there is no direct… Link to what to use.”

2. **Question Specificity & Iteration**: Many participants noted needing to repeatedly reword or clarify their questions before the chatbot provided a satisfying answer. This was apparent both for lockout scenarios and for MFA troubleshooting.

   - *Session 14* — “At first, it did not to me, but then I realized, oh, wait, I didn't put in all my information. So, at first, it did not make sense. It was just, to me, like, the generic response as I received before, but once I did put in that second portion of, oh, I did not receive the text message. The second answer that chatbot gave me, it made sense…”

3. **Accessibility & Navigation**: The only session involving a JAWS screen reader (Session 8) described substantial interface and navigation barriers, requiring manual assistance to tab/arrow through the chatbot and answer sections. Issues with identifying where to input questions, accessing past answers, and screen reader compatibility were highlighted.

   - *Session 8* — “When you've gotten to that point, and you… You're frustrated, and you… do have a live person. The last thing you want to do is start entering a whole bunch of other nodes. That's the last thing you… You're already frustrated… At his wit's end.”

4. **Information Overload vs. Sufficiency**: Most participants found the information “just right” (not overwhelming), aided by bullet points and links. However, some noted overwhelming long paragraphs, or wished for even more actionable, tailored suggestions when the scenario required.

   - *Session 14* — “It may be overwhelming a little bit, especially if… I'm a speed reader, so if I'm in a rush, or if I'm trying to get something, I could possibly see myself overlooking the important verbiage…”

5. **Dependents/Caregivers (Sessions 11, 12, 16, 17)**: These users were especially confused about eligibility, documentation, and how/if they could register for benefits or sign in as caregivers.

   - *Session 17* — “If the VA.gov has login techniques… And a majority, and then there are distinctly different techniques… to say, you know, for dependents…” (desired clear guidance categorizing logins by role).

---

## 9. Outcomes & Successes

- **High satisfaction** with general chatbot usability, especially for primary account issues.
- **Strong alignment with KPIs**, especially resolution rate and user satisfaction.
- **Constructive criticism** led to actionable recommendations in accessibility, clarity, and technical help loops.

## Overall Detailed Analysis

- **General Sentiment**: Overwhelmingly positive on ease of use, helpfulness of information, and appreciation for bullet-point formatting and links. Most found cognitive load manageable, and interface readable/scannable, even by those with vision impairements (except JAWS-user in Session 8, who faced major obstacles).
- **Role-based Issues**: Veterans had a generally smooth experience, while dependents/caregivers required more specific guidance. Most confusion arose with role-eligibility and which login to use, suggesting stronger role-specific onboarding is needed.
- **Multi-factor/Auth Issues**: Few understood the differences between available login methods and MFA, and most needed to be guided through step-by-step troubleshooting. The best answers were those that provided lists and explicit options (e.g., “You can reset your password here, or call this number…”).
- **Accessibility Challenges**: The only participant using a screen reader (Derek Pollett, Session 8) reported that having a live person as fallback was essential, navigation by tabbing was slow and often misdirected, and more explicit prompts (e.g., sound cue on focus) would dramatically improve experience for visually impaired veterans.
- **User Mental Models**: Many phrased their questions conversationally (not always matching search intent), expecting the chatbot to infer context. Most recognized quickly that the more details they provided, the better/clearer answer they received (“Chatbot only gives as much as you give it”). Some expressed annoyance at this, preferring one-and-done answers.
- **Trust and Security**: All users appreciated privacy disclaimers, most refused to enter personal info on principle, and several wanted reassurance that responses were up-to-date and accurate for important matters (benefits, health, locked accounts).
- **Expected Placement**: The majority expect the chatbot widget in the bottom right corner (industry standard), or “prominent” top placement. Some suggested persistent/floating widget for access from any page.
- **Comparisons**: Most found the VA chatbot experience as good or better than other chatbots (ChatGPT, customer service bots, etc.), citing faster, clearer answers.

---

## 10. Conclusion

**Overall, participants across diverse backgrounds found the VA.gov chatbot to be accurate, clear, and generally easy to use—averaging 4.7+ out of 5 on all ratings.** Bullet points, links, and a direct format were popular. The main areas for improvement are accessibility (especially for visually impaired and less tech-savvy users), the need for less generic clarifying questions, and expanded coverage for caregivers/dependents and edge case scenarios such as account lockout loops. Opportunities exist to expand demographic representation and real-world scenario testing, including additional language support, mobile/browser compatibility, and rural/low-bandwidth user scenarios.

## Key Recommendations

1. **Clarify Login Options**: Give clear, role-specific guidance (vet vs. dependent vs. caregiver), possibly by a “wizard” or quick-start role selector.
2. **Encourage Specific Questions**: Educate users (“The more detail you provide, the better the answer!”), perhaps with inline examples.
3. **Accessibility Improvements**: Add explicit sound prompts, focus cues, and keyboard navigation guidance; improve compatibility with major screen readers.
4. **Make Chatbot More Prominent**: Persistent widget floating on corner; adjustable box size for readability.
5. **Reduce Overload, Strengthen Formatting**: Continue using bullet points, separate/link out long content; offer options for more/less detail.
6. **Fallback to Human Help**: Make “talk to agent” or “call support” obvious after failed chatbot scenarios (especially for lockout or authentication).
7. **Strengthen Privacy & Security Messaging**: Keep disclaimers up front and clear; reassure about data safety.

### Data Source Statement
All findings above are grounded in detailed review and fact-checking against the original 17 transcripts, the provided research script [Conversation Guide](https://github.com/department-of-veterans-affairs/va.gov-team/blob/master/products/virtual-agent/research/2025-10-Chatbot%20User%20Testing%20Round%202/ConversationGuide(PerigeanVets).md), and the provided KPI dashboard in Domo. No stray values, range artifacts, or unconfirmed statements were included.

This summary is based exclusively on explicit, quoted statements and interaction patterns observed in the transcripts you uploaded. Numerical rating data is directly calculated from all available 1–5 scale responses (no interpolated, inferred, or "stray range" artifacts included). Role descriptions and scenario details reflect direct participant comments. Ambiguous/conflicting statements are either noted in context or omitted, and every quote is attributed to a specific session and participant when possible.

If further detailed synthesis or breakdown session-by-session is required, the source data is available and can be referenced for direct quotes or tables.

---

**Contact for follow-up:** Please specify if more granular analysis or breakdown (per scenario or persona) is needed.


