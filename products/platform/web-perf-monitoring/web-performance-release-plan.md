# Website Performance Monitoring

---

## Phase I: HCA Website Performance Testing

### Planning:
- Desired date range or test duration: Feb 2020 - March 2020
- Desired number of users: 2 Users
- How you'll conduct the testing: Create a full team setup of performance metrics for HCA Team including performance budgets and setting up alerts in Slack
- How you'll give the test users access to the product in production w/o making it live on VA.gov: N/A

### Results:
- Number of users: 2 Users 
- Number of bugs identified / fixed: 5/5
- Was the data submitted (if any) easy for VA to process?: The data is stored in speedcurve right now, but we can export it to VA whenever we have a data storage option. Alerts are being sent to VA Slack and email summaries are going to the admin users.
- Types of errors logged: Dashboard Setup issues, Team user management setup
- Any UX changes necessary based on the logs, or feedback on user challenges, or VA challenges? N/A
- If yes, what: N/A

## Phase II: Pilot Testing with Facilities Team

### Planning:
- Desired date range: 03/13/20 - 04/28/20
- Desired number of unique users: 2
- How you'll make the product available in production while limiting the # of users who can find/access it: N/A
- "Success" criteria (by the numbers):[use your KPIs to help guide this. It could be things like abondomnent rate < 20%, reported contact center calls < 2 calls, error rate < 5%, etc.]
  - Full set of checks run weekly for one url
  - Performance Budgets are set for every metric
  - Performance Budget alerts are successfully sent to Slack
  - Number of performance budget alerts sent
  - Weekly report of all metrics that exceed performance budgets 

### Results:
- Number of unique users: 2 Users
- Actual results
  - [ ] Full set of checks run weekly for one url
  - [ ] Performance Budgets are set for every metric
  - [ ] Performance Budget alerts are successfully sent to Slack
  - [ ] Number of performance budget alerts sent
  - [ ] Weekly report of all metrics that exceed performance budgets 
- Was the data submitted (if any) easy for VA to process?: Alerts are being sent, Weekly email alert is being sent. Data is not being stored anywhere currently.
- Types of errors logged: Onboarding & Setup, Offboarding
- Any UX changes necessary based on the logs, or feedback on user challenges, or VA challenges? N/A
- If yes, what: N/A


## Go Live!

### Planning:
- Desired date: ?
- Post-launch KPI 1: VSP has setup 100% of VFS teams in Speedcurve
- Post-launch KPI 2: # of metrics that exceed performance budgets
- Post-launch KPI 3: Performance gains per team
- etc
- Go / No Go: (ready / not ready)[https://github.com/department-of-veterans-affairs/va.gov-team/blob/master/platform/product-management/go-no-go-meeting-template.md]

### 1-week results:
- Number of unique users: x
- Post-launch KPI 1 actual: xx lorem ipsum
- Post-launch KPI 2 actual: xx lorem ipsum
- Post-launch KPI 3 actual: xx lorem ipsum
- Any issues with VA handling/processing?: yes/no, lorem ipsum
- Types of errors logged: lorem ipsum
- Any UX changes necessary based on the logs, or feedback on user challenges, or VA challenges? yes/no 
- If yes, what: lorem ipsum

### 1-month results:
- Number of unique users: x
- Post-launch KPI 1 actual: xx lorem ipsum
- Post-launch KPI 2 actual: xx lorem ipsum
- Post-launch KPI 3 actual: xx lorem ipsum
- Any issues with VA handling/processing?: yes/no, lorem ipsum
- Types of errors logged: lorem ipsum
- Any UX changes necessary based on the logs, or feedback on user challenges, or VA challenges? yes/no 
- If yes, what: lorem ipsum

## Post-launch Questions 

_To be completed once you have gathered your initial set of data, as outlined above._ 

1. How do the KPIs you gathered compare to your pre-launch definition(s) of "success"?
1. What qualitative feedback have you gathered from users or other stakeholders, if any?
1. Which of the assumptions you listed in your product outline were/were not validated? 
1. How might your product evolve now or in the future based on these results?
