Participant 2, Tuesday October 8, 2:00pm ET

### Role: 

- Lead for the platform, with special attention to the analytics team when possible.
- As a former PM, has thoughts about making PMs use quant and qual for their products

### General product health questions:

**What decisions would analytics data help you make?**

Qualitative or quantitiative user research: 
- Prioritize features, what needs improvement. What is the minimum viable step for users to get from point A to point B in the product
- Most important user feedback: “I cannot complete the tasks”
- Call Center: is qualitative feedback of “I cannot complete the task”
- Quant: Any statistically significant change or problem or making generalizations for the problem
- Would start with something in the funnel and if there are any red flags, like dropoff points, to dig deeper
- If I got all the data in one page, it would be overwhelming
- If my users couldn’t complete a task, I’d click to learn more, like heatmaps, why they’re leaving surveys. If it’s at the completion stage, is it something at the VA stage.
- How do I affect change in my product at Veteran Facing Services level

**Once you had the data, what sort of action would you take?**

- It depends what the data tells me: if I can’t find a specific thing, I’d do usability testing or further research on that page. The amount of information I can find in that step fo the process

**How often would you ideally look at analytics data? Monthly? Weekly? Daily?**

- Certain thresholds, like spike in call center calls about my product, should be a push notification. (A spike above what the normal Call Center amount of calls are). List of sample feedback organized around certain.
- Checking once or twice a week, sprint planning: would consult that as a thing to double check for prioritizing above new feature work. Would check before the monthly leads meeting to flag any problems with my product.

**How is my product healthy:**

- Completion rates when going through a bunch of different stakeholders. Call Center information for qualitative piece for total breakdown of what users need help for. 

### Prototype:

- First, a lot of the description stuff is good for other people, but I would be familiar with the headlines already. If this were my product, I’d probably have tweaks to make to the description.
- I’d see the blue links and am wondering if it’s a design system.
- I think it’s cool to pay attention to a few things (in the form section). I’d be worried about ignoring things that don’t need help.
- The all-charts-in-one. Hard to understand what the biggest problems are. Feels like a lot of stuff that’s generally trending together. I don’t see a spike or indicator because I don’t need the overarching data.
- The meter makes me nervous. I’d click on the “percent difference from target” to see what went wrong. I’d probably pick the data point with the biggest percent change number because I don’t know how they’re important in relation to the metered number. I’d need a little more guidance to understand.
- Looking at the data, I’d want to know what’s on fire as opposed to trends. 
- Funnel conversion: want to see this the most to get a sense of what’s going on during each step, and to see the actual feedback from call centers, not the numbers (anything that can aggregate the actual complaint)

**What would be important to you as leadership to keep an eye on multiple products?**
- The current Google Sheet MVP is close to that. At a glance high level completion rates. The teams that are involved. Would want to know a little bit more about which team is responsible for the product to provide support for teams who are struggling in certain areas. Would go to the DSVA product owner in each team.
- As leadership, I’d be a little nervous or want to know how we’re providing combined product scores. But would also want a dashboard of all of these together to quickly see who needs support.

**How would you combine the product score?**
- Not sure, would recommend talking to data people. Might separate out some things that are problems vs things doing well. Separate out things that are good vs things that are bad so they aren’t cancelling each other out. 
- Would want to combine it around similar themes.
- Would want more editorial feedback around things combined. Think it should come from Analytics & Insights. Would be worried about changing the goal from product to product. Would want to know changes across agreed upon minimums.
- Online submissions is less of a clear indication of product health. 

**How would you make product decisions after this?**
- Completion rates from start to finish. Call Center spikes quantitatively. Downtime. Maybe customer satisfaction rating, but I don’t really trust how uniformly we’re collecting that.
- I’d talk to the particular product person for why their completion rates are worse than others.
- Is one team struggling more than another, and do they need more help? Guiding through what points are especially challenging on my product. 
- Leadership: more likely to keep an eye on it to monitor different products
- Get people to think about it as not “extra information” which is what a lot of people think about “Call Centers or quantitative” but more crucial information for their product decisions
- As platform person, I wouldn’t be using it as much.
- As a DSVA leadership person, I’d be using it at least once a week. I’d look at it before syncs on Mondays. I’d look at it before meeting with each team. I’d use it to help me respond to random data calls. And when discussing with certain teams about taking on a big project and renewing a team’s contract.

### Other Questions:
- I didn’t catch “Percent difference from target”. I’d want a tooltip for each target to get some more information on what they mean. 
- What do we show everyone all at once vs. getting them to dig in more.
- What are the best pieces of information to show why certain things aren’t doing well. 
