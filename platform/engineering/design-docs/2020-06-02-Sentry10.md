---- 


# We've moved our docs!

### This document is no longer maintained.

### Please visit the [Platform website](https://depo-platform-documentation.scrollhelp.site/) for the latest information or contact the Platform Support Team via [#vfs-platform-support](https://dsva.slack.com/archives/CBU0KDSB1).


----
# Sentry 10
 
**Author(s):** Bill Ryan

**Last Updated:** June 05, 2020 

**Status:** Draft | **In Review** | Approved

**Approvers:**

- [x] Wyatt Walter
- [ ] Nathan Hruby
- [x] Keifer Furzland

## Overview

### Objective
We want to answer the following questions:

- Why do we want Sentry 10?
- How do we build & deploy Sentry 10
- What AWS infrastructure do we use?
- How is Sentry 10 configured?
- How (if at all) is Sentry 10 migrated from, or related to our existing Sentry deployment?

The intended audience is other software engineers, particularly those with experience in our [devops repository](https://github.com/department-of-veterans-affairs/devops).

### Background
We current build, release, and deploy Sentry 9.1.2. There are some features like Github issue tagging, and release tracking, and issue owners that we think would help compartmentalize the features of each team into their own "part" of Sentry, with the idea that teams would have a better chance at owning their respective sections.

### High Level Design

Sentry 10 will be a new deployment in our BRD cycle, with it's own build, release, and deploy jenkins jobs.

It will be similar to Prometheus in the way it deploys. It will be a single, large EC2 instance using an EBS volume that will persist between deployments. [A slack thread discusses this here](https://dsva.slack.com/archives/CJYRZK2HH/p1588272467198700)

We will use the [`getsentry/onpremise`](https://github.com/getsentry/onpremise) repo to help bootstrap Sentry 10 onto the EC2 instance.

The EC2 instance will be sufficiently large since it will be responsible for handling 100% of Sentry traffic.


## Specifics
### Detailed Design

The [`getsentry/onpremise`](https://github.com/getsentry/onpremise) repo utilizes 18 docker images to run Sentry 10:

```
sentry_onpremise_clickhouse_1               /entrypoint.sh                   Up      8123/tcp, 9000/tcp, 9009/tcp
sentry_onpremise_cron_1                     /bin/sh -c exec /docker-en ...   Up      9000/tcp                    
sentry_onpremise_kafka_1                    /etc/confluent/docker/run        Up      9092/tcp                    
sentry_onpremise_memcached_1                docker-entrypoint.sh memcached   Up      11211/tcp                   
sentry_onpremise_post-process-forwarder_1   /bin/sh -c exec /docker-en ...   Up      9000/tcp                    
sentry_onpremise_postgres_1                 docker-entrypoint.sh postgres    Up      5432/tcp                    
sentry_onpremise_redis_1                    docker-entrypoint.sh redis ...   Up      6379/tcp                    
sentry_onpremise_sentry-cleanup_1           /entrypoint.sh 0 0 * * * g ...   Up      9000/tcp                    
sentry_onpremise_smtp_1                     docker-entrypoint.sh exim  ...   Up      25/tcp                      
sentry_onpremise_snuba-api_1                ./docker_entrypoint.sh api       Up      1218/tcp                    
sentry_onpremise_snuba-cleanup_1            /entrypoint.sh */5 * * * * ...   Up      1218/tcp                    
sentry_onpremise_snuba-consumer_1           ./docker_entrypoint.sh con ...   Up      1218/tcp                    
sentry_onpremise_snuba-replacer_1           ./docker_entrypoint.sh rep ...   Up      1218/tcp                    
sentry_onpremise_symbolicator-cleanup_1     /entrypoint.sh 55 23 * * * ...   Up      3021/tcp                    
sentry_onpremise_symbolicator_1             /bin/bash /docker-entrypoi ...   Up      3021/tcp                    
sentry_onpremise_web_1                      /bin/sh -c exec /docker-en ...   Up      0.0.0.0:9000->9000/tcp      
sentry_onpremise_worker_1                   /bin/sh -c exec /docker-en ...   Up      9000/tcp                    
sentry_onpremise_zookeeper_1                /etc/confluent/docker/run        Up      2181/tcp, 2888/tcp, 3888/tcp
```

For comparison sake, our Sentry 9 uses 3 docker containers:

- web
- worker
- cron

#### Snapshots

Currently, snapshots are taken while the instance is running. For Sentry 10 however, this will need to change in order to keep state consistent between deploys. This may require an alternate ansible deployment code path.

### Code Location
All code will live in the [`devops` repo](https://github.com/department-of-veterans-affairs/devops)

### Testing Plan
We will stand up Sentry 10 in parallel with our existing Sentry 9.1.2. Then we will:

1. Configure the `Settings.sentry.dsn` of `dev` to use the Sentry 10 deployment
2. Run a load test against dev aimed at sending lots of exceptions to Sentry & observe performance

Once these two steps are performed, deployment would entail one of the following:

1. Deploy Sentry 10 to `prod` and keep Sentry 9 on standby if things start failing
2. Configure a portion of the `vets-api` instances to point to 9 and the others to 10

#### Multiple Configured Sentry DSNs
Another idea is to configure multiple Sentry DSN's and send all our errors to both Sentry 9 & 10. This feature doesn't appear to be supported by the [raven-ruby gem](https://github.com/getsentry/raven-ruby), so we would have to fork the gem and do the work ourselves.

### Logging
N/A

### Debugging
N/A - I think?

### Caveats
_To be determined._

### Security Concerns
No additional security concerns to the ones we have with our existing Sentry.

### Privacy Concerns
No additional privacy concerns to the ones we have with our existing Sentry.

### Open Questions and Risks
#### 1. Performance Scaling
Since we intend on deploying to a single EC2 instance, there is not much room for scaling up in the event that Sentry 10 runs into CPU or memory bottlenecks.

#### 2. Moving Away from `getsentry/onpremise` Happy Path
The [`getsentry/onpremise` repo](https://github.com/getsentry/onpremise) is a great way to quickly run your own "on premise" Sentry 10 application. It was not designed with scale in mind. In this design, we will be heavily relying on this bootstrapping repo for building and deploying. It's likely that we will eventually have a need to do something custom that would require us to fork the `getsentry/onpremise` repo, making any future modifications and upgrades difficult.

#### 3. Data Backups
Our existing Sentry uses RDS which allows us to create daily snapshots. For Sentry 10, it will use the `postgres` docker image (at least at first), since using RDS would be the first step in moving away from the `getsentry/onpremise` bootstrap happy path.

#### 4. Downtime Between Deployments due to EBS Volumes

For prometheus, about 1.5 minutes of downtime occurs for each deployment - Sentry will experience a similar downtime. This is the time necessary for the EBS volume to detatch and re-attach from the old to new instance respectively. It also observes the amount of time required to stop and start the respective service (Sentry docker-compose in this case).


### Work Estimates
#### Phase 1 - Ansible Build
Build the AWS AMI. In this step, all docker images should be downloaded onto the machine. This would be considered the ansible "build".

#### Phase 2a - Ansible Deployment
Create the ansible code to deploy the Sentr10 AMI. This might just be a deployment configuration or it might require new deployment ansible roles or modification to existing roles.

#### Phase 2b - AWS Deplyoment Infra (terraform)
Create the necessary terraform code to build the AWS infrastructure needed for deploying into our `vetsgov-utility` VPC.

:warning: There's a bidirectional dependency on the ansible deployment scripts & aws infra needed - sometimes you don't know which infrastructure is needed until you have the deployment config and sometimes you don't know which deployment configuration is needed until you know what infrastrucutre is available. :warning:


#### Phase 3 - Observe, Modify, Iterate
In this phase, we observe the performance of our Sentry 10 instances. If performance is not adequte, we can begin replacing some of the docker containers with dedicated AWS infrastructure. See #Future-Work


### Alternatives
In the first iteration of Sentry 10, the plan was to use multiple instances behind a load balancer. [You can see that design here](https://github.com/department-of-veterans-affairs/va.gov-team/blob/ccd761052fc3bc32f8c0c15689e2d7deccc900e0/teams/vsp/teams/tools/backend/sentry_design.md). However, there are [not enough available IP addresses in the `vagov-utility` VPC](https://github.com/department-of-veterans-affairs/va.gov-team/issues/1402) to house a load balancer.

![image](https://user-images.githubusercontent.com/3077884/79232599-b861b900-7e35-11ea-8c5b-63b46f67cca4.png)

As a result, the new design does away with the load balancer entirely in favor of a singular Sentry10 EC2 instance.


### Future Work

#### Near Future

We'd like to tease out components from the `docker-compose` YAML file defined in the [`getsentry/onpremise`](https://github.com/getsentry/onpremise) bootstrap repo, and instead use AWS infrastructure.

In the near future, this would be using redis elasticache & postgres RDS instead of the redis and postgres images in the docker-compose.yml. This would benefit us by:

- allowing us to use AWS features, such as snapshots and duplicate redis nodes
- more performance scalability
- more flexibility to independently control & monitor

A big caveat to this is working with [`getsentry/onpremise`](https://github.com/getsentry/onpremise) repo owners to support this feature. We *do not* want to go off their happy path.

#### Distant Future
It would be ideal to completely remove our dependence on [`getsentry/onpremise`](https://github.com/getsentry/onpremise), and instead have each piece of the necessary infrastructure live in AWS or our own docker container.



### Revision History

Date | Revisions Made | Author
-----|----------------|--------
June 02, 2020 | Doc Creation | Bill Ryan
June 05, 2020 | Edits inspired by peer review comments | Bill Ryan
June 09, 2020 | Peer review comments from approval meeting | Bill Ryan
