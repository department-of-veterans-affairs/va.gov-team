---- 


# We've moved our docs!

### This document is no longer maintained.

### Please visit the [Platform website](https://depo-platform-documentation.scrollhelp.site/) for the latest information or contact the Platform Support Team via [#vfs-platform-support](https://dsva.slack.com/archives/CBU0KDSB1).


----
# CMS Content Caching Strategy

**Author(s):** Eugene Doan  
**Last Updated:** April 28, 2021  
**Status:** **Draft** | In Review | Approved  
**Approvers:**
- [ ] Tim Wright
- [x] Demian Ginther
- [ ] Michael Fleet
- [ ] Dror Matalon

## Overview

### Objective

Systems and workflows that need to access the latest CMS content should be able to do so without advanced configuration or permissions (e.g., SOCKS proxy).
Having this access would benefit review instances, the GitHub Actions build pipeline, and local development. There may also be other use cases.
The intended audience for this document are both users and maintainers of these systems and any others that have a dependency on CMS content.

To support this functionality, this document will outline an approach to setting up a cache of the latest CMS content.
- This not intended to form a sophisticated caching policy with version management and rollback capabilities.
- Instead, the focus is on keeping the latest version the the CMS content updated on a schedule.

This document will not delve into implementation details of how various systems (builds, local development, etc.) will use the cache.

### Background

A large part of the VA.gov website is static content that is derived from a Drupal-based content management system (CMS). 
Building the static pages of the site involves fetching the content either directly from the Drupal CMS or from a cache in Amazon S3 (Simple Storage Service).

The CMS is hosted in the internal VA network, so the request for its content has to traverse the Trusted Internet Connection (TIC).
- Only systems within the network have direct access to internal services like the CMS.
- External systems, such as local development machines outside of the network, can only access internal services through a SOCKS proxy.
- Publicly hosted services such as review instances or GitHub Actions workflows currently have no means of accessing internal services.

To circumvent the need for the SOCKS proxy or a direct internal connection, there is a content cache that's regularly generated by the Jenkins build pipeline.
- The build for each environment (dev, staging, prod) tries to pull the content to fully build the website.
- If the Drupal content pull is successful, the pipeline caches that content in S3. Otherwise, the build uses a previous cache.
- The cache is stored with a key derived from a combination of the environment name and a hash of the GraphQL query that's used to fetch all pages from Drupal.

There have been issues with generating the cache key (department-of-veterans-affairs/va.gov-team#16401) that forced review instances to attempt to directly pull content from Drupal and fail (department-of-veterans-affairs/va.gov-team#16637).
- Review instances are staging-like environments, with website and API integration, used by many veteran-facing services (VFS) teams to test changes within feature branches.
- From the service interruption, it was explained that review instances are in a separate virtual private cloud (VPC) than the internal VPC that hosts internal services like the CMS.
- In addition, because the review instances have configured an Internet gateway, they do not have clearance from VA to peer or otherwise connect with the internal VPC.
- Since they don't have a SOCKS proxy configuration or direct access to the CMS, review instances rely entirely on the cache to build the environment.

The problem with review instances prompted an overarching discussion around revising the content caching strategy.
The goals of that change would be to prevent similar issues and to better support systems that depend on the cache.

### High Level Design

Content for builds on `main` are cached in S3 with a key such as `content-cache/main/content.tar.gz` instead of an ever-changing hash-based key.

An Amazon Web Services (AWS) Lambda function pulls the content from Drupal and sync the cache periodically (every 5 minutes).
- The function is versioned so that branches that have modified the query for the content can access the corresponding data.
- Feature branches that change the query have their builds cache content to `content-cache/<function_source_hash>/content.tar.gz`.
- The hash in the S3 key corresponds to the checksum of the function source.

Local development environments, review instances, GitHub Actions, and any other systems can use this cache as needed.

## Specifics

### Detailed Design

#### Lambda Function

Using parts of the current Drupal API client code in the `vets-website` repo, the Lambda function is built with Webpack.

It downloads the Drupal data into the following directory structure. This matches the cache that's created in `.cache/<buildtype>/drupal` from content builds.

```
drupal
|__ downloads           // Assets, split into images and other files.
|  |__ files
|  |__ img
|__ feature-flags.json  // Feature flags set in Drupal.
|__ pages.json          // JSON data retrieved from the GraphQL query.
```

The function performs the following steps:
1. Fetch Drupal data with the GraphQL query.
   - This leverages the content build's Drupal API client (`src/site/stages/build/drupal/api.js`) and the build options helpers (`src/site/stages/build/options`) to set default values for that API client.
   - Gathering build options automatically sends a request to the CMS for feature flags and generates that file on disk at `/tmp/.cache/localhost/drupal/feature-flags.json`.
1. Generate a tarball in memory and store the retrieved data as `pages.json`.
   - While the content build typically creates the cache on disk, there is a [512 MB limit](https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html) for temporary disk storage in the Lambda environment.
   - Since there is more space in memory (3008 MB for functions in Gov Cloud, but 10 GB in other regions), the cache will be built in memory before uploading to S3.
1. Store `feature-flags.json` as itself in the tarball.
1. Parse the CMS URLs (internal ALB URL or `*.cms.va.gov`) in `pages.json` using regex.
   - This will work similarly to the content build pipeline step of downloading Drupal assets.
   - To avoid running out of memory from having too many files open at once, we limit the number of concurrent downloads to 10 for now.
   - Assets are downloaded into the `downloads` directory, separated into either `files` or `img` based on the file suffix.
1. Close the tarball for writing and compress it.
1. Upload the archived and compressed cache to the bucket.
   - For `main`, the key is `content-cache/main/content.tar.gz`.
   - For feature branches, the key is `content-cache/<hash>/content.tar.gz`, where the hash is the checksum of the source ZIP file.

#### Continuous Integration

As part of continuous integration (CI), the function source is zipped and uploaded to AWS in S3 (or ECR) as the function source.
- Build the function and zip it.
- Get the checksum or hash of the ZIP file.
- Store the ZIP file in an S3 bucket at a key corresponding to the checksum if it doesn't already exist.

The S3 key will be `vetsgov-website-builds-s3-upload/content-cache/main/function.zip` on the `main` branch. Feature branches will replace `main` in that key with the checksum of the ZIP file.

Check the [list of function aliases](https://docs.aws.amazon.com/lambda/latest/dg/API_ListAliases.html) for an alias matching the checksum.
If an alias doesn't exist yet for the checksum:
- [Update and publish the function code](https://docs.aws.amazon.com/lambda/latest/dg/API_UpdateFunctionCode.html) using the archived function from S3.
- [Create the alias](https://docs.aws.amazon.com/lambda/latest/dg/API_CreateAlias.html) for the new version of the function.

If the current branch is **not** `main`, [synchronously invoke](https://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html) the new version of the function (aliased with the checksum).

If the current branch is `main`, also get the version that the `main` alias points to. If the main alias doesn't match the version of the checksum alias:
- [Update the `main` alias](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/lambda/update-alias.html) to the version currently aliased with the checksum.
- Synchronously invoke the new version of the function (aliased with `main`).

Pull the cache from S3 that corresponds to the checksum (or `main` if on the `main` branch) for the build step.

### Code Location

Initially, the Lambda function and its supporting code will live in the `vets-website`.
Once the content build is separated into the `content-build` repo, they will be moved there.

Handler source code, bundling, and upload (`vets-website`):
- Lambda function: `src/platform/lambdas/content-cache.js`
- Webpack: `config/webpack.content-cache.js`
- GitHub Actions: `.github/workflows/continuous-integration.yml`

The Lambda function will be provisioned in the `devops` repo.
- Terraform: `terraform/environments/dsva-vagov-utility/lambda.tf`

### Testing Plan

The GitHub Actions pipeline will include a job for building and updating the function. This will alert us to any issues with building the function and provisioning it in AWS.

The prototype for the function will simply download the data from the Drupal server. This is to verify that it can connect to the CMS at all.
Once that is confirmed, the function will be further developed to cache the downloaded data to S3.

When the function is fully written, we will monitor the log output in AWS CloudWatch to observe whether it works correctly.

### Logging

GitHub Actions will output logs for the build and versioning of the function.
Log output from the function (from calling `console` methods) will be stored in CloudWatch.

### Debugging

One can start diagnosing any issues using the logs mentioned above.

The Webpack build of the function can be run locally to debug issues with the build.

To locally validate that the code properly pulls the Drupal data, the function can be run in a script with some modifications to disable the parts that are specific to the Lambda environment.

Once the data is cached, it can be downloaded and used to build content. That build can be compared with a content build that pulls fresh data from Drupal with a SOCKS connection.

### Caveats

There are no known caveats at this time.

### Security Concerns

The function is not expected to take much bandwidth on the Drupal server, so there should be no concern about denial of service. It will be scheduled to make a request only every 5 or 10 minutes.

Without any user input, it should not be susceptible to malicious requests either.

### Privacy Concerns

There are no privacy concerns as no user data is involved.

### Open Questions and Risks

What's the appropriate frequency to update the content cache?
- Previously considered 10 minutes, because the local download of the data with SOCKS proxy seemed to take roughly 11 minutes.
- Since the function should be able to directly connect to the Drupal server, it takes less time than local development.
- From testing the prototype, it takes much less time (a little more than 2 minutes), so it seems that 5 minutes would be feasible.
- This can be reduced further if teams find it valuable to have more frequent updates.

How do we want to manage the bucket storing the function source?
- Do we want to periodically clean out old functions?
- How will the storage costs grow as we keep old versions of the function around?
- As we anticipate the release of more content, there may be many changes to the function over time.
- We do not currently clean up the bucket for the `vets-website` builds, and that certainly occupies more space by many orders of magnitude.

Is it worth implementing the cache for feature branches if we're able to run a content build without it?
- It's possible to configure a self-hosted runner in GitHub Actions to allow the build to access the Drupal server directly.
  - In that case, the cache would not be the only means of building content.
- Having a cache for every branch would be less valuable than for `main`.
  - Large cost in resources and maintenance to scale the updates across all branches.
  - Directly pulling content makes more sense (and is a lighter process) for active development in branches that change the query.
- What are the implications for review instances (or other use cases) if they always reflect content from the GraphQL query that's in `main` and not from branches?
- Perhaps content can still be cached by directly putting it in S3 with a key that corresponds to the checksum of the changed function source.
  - This would be instead of uploading a new version of the function and invoking that version of the function.
  - Essentially, this changes the hash key for the content from (A) the GraphQL query string to (B) the generated caching function source code.
- The work for implementing versioning for the function should involve decisions to resolve these questions.

### Work Estimates

- Implement versioning for the function (5 pts)
- Validate the cached content and versioning logic (3 pts)
- Use the new cache to run the content build in GitHub Actions (2 pts)
- Use the new cache in review instances (2 pts)

### Alternatives

#### Setting up a Terraform configuration for the Lambda function source in the `devops` repo.

The function source has dependencies on parts of the content build, so it would be cumbersome to integrate into the `devops` repo.

#### Using Serverless Framework or Terraform to publish the Lambda function in the `vets-website` repo.

Since we're provisioning a single function under the ownership of the FE Tools team for now, it seems unnecessary to leverage frameworks like Serverless or Terraform.

There are some very specific versioning rules in the design that may not be straightforward to implement within these frameworks.

### Future Work

The content caching workflow should be moved to the `content-build` repo when the content build separation is complete.

### Revision History

Date | Revisions Made | Author
-----|----------------|--------
April 28, 2021 | Reverted to draft status to close. | Eugene Doan
April 23, 2021 | Updated after completing prototype. | Eugene Doan
March 8, 2021 | Completed first draft for review. | Eugene Doan
March 5, 2021 | Initial draft | Eugene Doan
